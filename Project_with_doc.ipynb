{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basics of Mobile Robotics Final Project** \n",
    "## **Course: \"Basics of Mobile Robotics\"(MICRO-452)**\n",
    "## **Professor: Francesco Mondada**\n",
    "## **EPFL - December 2023**\n",
    "### **Authors:**\n",
    "- Evangelista Santiago Roberto\n",
    "\n",
    "- Giovine Angelo\n",
    "\n",
    "- He Weifeng\n",
    "\n",
    "- Syla Valmir\n",
    "\n",
    "## **Tables of contents**\n",
    "\n",
    "- [Demo Videos](#Demo-Videos)\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "  - [Project description and details](#Project-description-and-details)\n",
    "  - [Modules and libraries required](#Modules-and-libraries-required)\n",
    "\n",
    "- [Environment](#Environment)\n",
    "\n",
    "- [Vision](#Vision)\n",
    "  - [Camera](#Camera)\n",
    "  - [Environment mapping](#Environment-mapping)\n",
    "  - [Global obstacle detection](#Global-obstacle-detection)\n",
    "  - [Robot detection](#Robot-detection)\n",
    "  - [Goal detection](#Goal-detection)\n",
    "\n",
    "  \n",
    "\n",
    "- [Global Navigation](#Global-Navigation)\n",
    "  - [Visibility graph](#Visibility-graph)\n",
    "\n",
    "\n",
    "\n",
    "- [Local Navigation](#Local-Navigation)\n",
    "  - [Local obstacle detection](#Local-obstacle-detection)\n",
    "  - [Kidnaping](#Kidnaping)\n",
    "\n",
    "- [Filtering](#Filtering:-Extended-Kalman-Filter)\n",
    "  - [Why Kalman?](#Deriving-the-state-space-model-of-the-Thymio-Robot)\n",
    "  - [Deriving the state space model of the Thymio Robot](#Deriving-the-state-space-model-of-the-Thymio-Robot)\n",
    "  - [Dealing with the input](#Dealing-with-the-input)\n",
    "  - [Experimentation to derive $Cl$ and $Cr$](#Experimentation-to-derive-$Cl$-and-$Cr$)\n",
    "  - [Calcultating the covariance matrix of the motion model $Q_t$](#Calcultating-the-covariance-matrix-of-the-motion-model-$Q_t$)\n",
    "  - [Deriving the observation model of the Thymio Robot](#Deriving-the-observation-model-of-the-Thymio-Robot)\n",
    "  - [Calcultating the covariance matrix of the observation model $R_t$ ](#Calcultating-the-covariance-matrix-of-the-observation-model-$R_t$ )\n",
    "  - [Implementing the Extended Kalman Filter](#Implementing-the-Extended-Kalman-Filter)\n",
    "\n",
    "\n",
    "\n",
    "- [Control law ](#Control-law)\n",
    "\n",
    "- [Main](#Main)\n",
    "\n",
    "- [The Code](#The-Code)\n",
    "\n",
    "- [Sources](#Sources)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Demo videos**\n",
    "\n",
    "The optimal path is displayed in white, while the real-time estimated position of the Thymio, obtained through filtering, is indicated by a black dotted line.\n",
    "\n",
    "Full demo + Normal situation + Kidnapping + Camera obstructed + Local obstacles: https://youtu.be/rz_EOHwGMOw\n",
    "\n",
    "Full demo: https://youtu.be/DTvLO2GhbcM\n",
    "    \n",
    "Normal situation: https://youtu.be/M8M3lAdUMPQ\n",
    "\n",
    "Kidnapping: https://youtu.be/5M4zRgAHsiA\n",
    "\n",
    "Camera obstructed: https://youtu.be/vcZKryRRfx0\n",
    "\n",
    "Local obstacles: https://youtu.be/Q0z2UTtgOtY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "#### **Project description and details:**\n",
    "The project is structured around five distinct modules, each tailored to a specific aspect of the Thymio Robot's navigation and operational capabilities:\n",
    "- Vision Module\n",
    "- Global Navigation Module\n",
    "- Local Navigation Module\n",
    "- Filtering Module\n",
    "- Robot control\n",
    "\n",
    "The project uses mainly the camelCase naming convention for variables and functions (everything except the kalman filter). For the kalman filter, the snake case convention was used. This was because of the ditribution of the work among the team members.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modules and libraries required:**\n",
    "\n",
    "The following modules and libraries are required to run the code:\n",
    "- numpy\n",
    "- matplotlib\n",
    "- cv2\n",
    "- math\n",
    "- time\n",
    "- pyvisgraph\n",
    "- tdmclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment**\n",
    "\n",
    "For the environment, a monochromatic blue background was selected, complemented by 2D green obstacles and a yellow target area. The Thymio robot is identified by a distinctive red mask applied to it. The environment was constructed on a pool table.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/environment.jpeg\" alt=\"Environment on the pool table\" width=\"400\" style=\"margin-right: 10px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "ChatGPT-4 was used to create polygons as map obstacles. While the model excelled in generating shapes, it faced challenges with non-overlapping shapes in PDFs and random polygon generation. The details were then manually specified, making it a collaborative effort. This experience underscored the limitations of Large Language Models in creative tasks.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/GPT_error.png\" alt=\"GPT error message\" width=\"450\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/GPT_env.png\" alt=\"polygons generated by GPT4 \" width=\"550\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Camera**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/iPhone.jpg\" alt=\"very bad image of the concept\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "The camera used in the project was not the Aukey Stream Series camera provided by the course. The development started with the USB camera, but while experimenting, it was discovered that using a macbook, the camera of an iPhone with the same iCloud account is automatically recognized as a webcam wirelessly. This was great considering the camera had to be placed in the roof pointing down at the table. This allowed to stick the phone to the roof and not have any cables hanging from the roof to the computer. Moreover it was noted that the iPhone camera provided smoother images than the USB camera. From python, the camera is recognized as a normal webcam, so the code did not have to be changed. \n",
    "#### **Environment mapping**\n",
    "\n",
    "The computer vision module maps the environment to a 2d plane to allow for correct functioning of the following modules. To do this, four small green markers are placed on the corners of the table. The camera detects the markers (the global obstacles are also green so the external 4 green detected objects are taken) and maps these point to certain point on the plane depending on the aspect ratio of the markers.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/beforeMapping.jpg\" alt=\"Environment on the pool table from the camera before\" height=\"300\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/afterMap.jpeg\" alt=\"Environment on the pool table from the camera after\" height=\"300\"/>\n",
    "</div>\n",
    "\n",
    "### **Global obstacle detection**\n",
    "\n",
    "The global obstacle detection is done by detecting the green objects in the environment. These objects are filtered by size and are expanded since the global navigation algorithm used is the visibility graph and the size of the robot is not inherently considered.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/PHOTO_OBJECT.png\" alt=\"green obstacle on the table\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "### **Robot detection**\n",
    "\n",
    "The robot is found by detecting the red mask on the robot. The mask is a composed of a white background and two red rectangles, one large in the back (center between the two wheels, important since this is the center of rotation) and one small in the front. Two rectangles are used to be able to discriminate the angle from 0 to 2pi instead of 0 to pi. This method gives the robot's position and orientation.\n",
    "\n",
    "The Thymio detection can be seen in the figure from the previous section.\n",
    "\n",
    "\n",
    "### **Goal detection**\n",
    "The goal is detected using a yellow detector. The centroid of the largest yellow object is taken as the goal position.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/PHOTO_GOAL.png\" alt=\"yellow obstacle on the table\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Global Navigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visibility graph**\n",
    "For the global navigation, and like mentioned before, the visibility graph algorithm was used. To do this the library pyvisgraph was used. The library has all the functions needed, including the finding of the shortest path, so no extra libraries (apart from the already used numpy and matplotlib) were needed. It is important to note that the library uses a different coordinate standard than openCV, so an extra function was needed to convert the coordinates. Another important addition was the pushing far of the point in the edge. When initially testing the system issues when a global obstacle was close to the edge were encountered, even if the global obstacles were expanded considering the robot size. To resolve this the points of the global obstacles that are on the edge were pushed a large distance far away from the the edge outwards. This ensured that the path planning algorithm would not consider them.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/seenFromCam.png\" alt=\"Image seen from the camera\" height=\"300\"/>\n",
    "    <img src=\"img/pointsInPath.png\" alt=\"Plot with points in the shortest path\" height=\"300\" style=\"margin-right: 10px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Local Navigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Local obstacle detection**\n",
    "\n",
    "To detect the local obstacles the horizontal distance sensors in the front of the robot were used. Depending on in which side the obstacle is detected, the robot will turn in the opposite direction and avoid the obstacle using a squared path manouver. The moving of the robot was done using a direct speed input and timers, since it proved to be simple but effective. The values of the timers were found empirically. Even if considerably precise, at the end of the manouver and once the obstacle has been avoided, the global path planning algorithm is run again to ensure the continuation is smooth and the robot recovers in the best possible manner. This method has some limitations though. The aproximated size of the obstacles has to be known a priori and sufficient place to do the manouver is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kidnaping**\n",
    "\n",
    "\n",
    "To detect the kidnaping of the robot the vertical distance sensors on the robot were use. If the robot is lifted from the table a kidnaped flag is activated and while the robot is not positioned backed on the talbe it turns off the motors and waits. When the robot is placed back in the table, a few seconds are waited until the robot is stable and no other perturbations are present (e.g. hands that can be confused as the robot, causing problem with the path planning). In the end, the global path planning algorithm is run again to find the new path to the goal. \n",
    "\n",
    "The vertical distance sensors were used in the ground reflected mode. After experimentation it was remarked that this was the mode that showed the most difference when the robot was placed on the table or lifted from it, facilitating the determination of the threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Filtering: Extended Kalman Filter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Kalman?**\n",
    "\n",
    "During the course different Bayes filters were presented to estimate the state of a robot in a given environment. For example, a particle filter could have been used, but it is more computationally expensive and it is not necessary for this application. Considering the project requirements, the Kalman filter was finally chosen. This is a recursive filter that estimates the state of a system from a series of noisy measurements. It is a very efficient algorithm that can be used to solve the localization problem of a robot, but makes a fundamental assumption about the distribution of the noise of the various sensors. In fact, it assumes that the noise is Gaussian distributed, which is not always the case in practice.\n",
    "\n",
    "In the presented case, odometry errors can arise from various sources:\n",
    "\n",
    "- Sensor measurement errors: The sensors might not be perfectly accurate or might exhibit some drift over time.\n",
    "- Mechanical Misalignments: Slight imperfections in the robot's construction or wear in components can cause discrepancies.\n",
    "- Slippage Error: The wheels may slip or lose traction, leading to inaccurate distance measurements.\n",
    "\n",
    "Each of these error sources contributes in a small and independent manner to the general error. According to the central limit theorem, when a large number of small, independent effects are summed, their overall distribution tends to approach a Gaussian distribution. This implies that even if each individual source of error might not be Gaussian, their cumulative effect tends towards a Normal distribution. \n",
    "\n",
    "Considering the above, the Kalman filter seems to be a good choice for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deriving the state space model of the Thymio Robot**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/state_space_.png\" alt=\"2D image of a differential drive robot taken from Automatic Addison\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "First, the state and input variables need to be defined. The following state variables will be used: $x$, $y$ for the position, and $\\gamma$ for the orientation. Since the actuators of the Thyimio are controlled in speed, the input variables $v$ forward velocity, and $\\omega$ angular velocity can be defined. After some trigonometric analysis the state vector $s$ is defined as follows: \n",
    "$$\n",
    "s_{t} =\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{t-1} + v_{t-1}\\cos (\\gamma_{t-1}) \\cdot \\Delta t \\\\\n",
    "y_{t-1} + v_{t-1}\\sin (\\gamma_{t-1}) \\cdot \\Delta t \\\\\n",
    "\\gamma_{t-1} + \\omega_{t-1} \\cdot \\Delta t\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "f_1 \\\\\n",
    "f_2 \\\\\n",
    "f_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence the input vector $u$ is defined as follows:\n",
    "\n",
    "$$\n",
    "u_{t} = \n",
    "\\begin{bmatrix}\n",
    "    v_{t} \\\\ \n",
    "     \\omega_{t}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "If the system were linear it would be defined as:  $s_{t}= A_{t-1}s_{t-1} + B_{t-1}u_{t-1}$, where $A_{t-1}$ is the state matrix and $B_{t-1}$ is the input matrix. Since the system in the presented case is not linear, an approximation of the system can be made using the Jacobian matrix.\n",
    "The robot moves only when it receives instructions to rotate its wheels. As a result, in this scenario, the $A_{t-1}$ matrix is an identity matrix (linear relation). For the $B_{t-1}$ matrix the Jacobian aproximation matrix is used, which is defined as follows:\n",
    "\n",
    "$$\n",
    "B_{t-1} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial v_{t-1}} & \\frac{\\partial f_1}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_2}{\\partial v_{t-1}} & \\frac{\\partial f_2}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_3}{\\partial v_{t-1}} & \\frac{\\partial f_3}{\\partial \\omega_{t-1}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "\\sin(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "0 & \\Delta t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "If the process noise  $ w_{t} = [ w^1_{t}, w^2_{t}, w^3_{t} ] $\n",
    " is also considered, the state space model can be then defined as:\n",
    "$$s_{t}= A_{t-1}s_{t-1} + B_{t-1}u_{t-1} +w_{t-1}$$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{t-1} \\\\\n",
    "y_{t-1} \\\\\n",
    "\\gamma_{t-1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "\\sin(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "0 & \\Delta t\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{t-1} \\\\\n",
    "\\omega_{t-1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w^1_{t-1} \\\\\n",
    "w^2_{t-1} \\\\\n",
    "w^3_{t-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that it is assumed that the process noise is zero-mean Gaussian noise with covariance matrix $Q_k$, which was computed empirically with a process explained in the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dealing with the inputs**\n",
    "\n",
    "It is important to note that the state space model is controlled in $v$ forward velocity, and $\\omega$ angular velocity. In reality only the speed of the two motors $v_{right}$, and $v_{left}$ of the Thymio can be controlled. A conversion between the speed of the robot $v$ and $\\omega$ into $v_{r}$, and $v_{l}$ is then derived using the following equations:\n",
    "\n",
    "$v_{r} = \\frac{2v+\\omega L}{2R}$ , $v_{l} = \\frac{2v-\\omega L}{2R}$\n",
    "\n",
    "Where $L$ is the distance between the two wheels and $R$ is the radius of the wheels.\n",
    "\n",
    "In order to control the robot correctly, the multiplication factors $C_r$ and $C_l$ are added to the equations to account for the unit change between the speed and the inputs of the robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experimentation to derive $Cl$ and $Cr$**\n",
    "\n",
    "In order to use the $Cl$ and $Cr$ values in the program, they need to be obtained empirically. This section contains the collected data and the results. First, each wheel of the Thymio was marked. Then, the robot was fixed at a constant speed $spd = 200$  $Thymio units$ and the number of revolutions of each wheel in a fixed time interval $t$ were measured. The experiment was done 10 times and the average values of $Cl$ and $Cr$ were calculated.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Thymio_wheel.jpeg\" alt=\"Description of Thymio wheel\" width=\"500\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/Cl_Cr.png\" alt=\"Description of second image\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "$$Cl = 69.33821285 \\frac{s}{rad}Thymio units,Cr = 70.74580573 \\frac{s}{rad}Thymio units$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calcultating the covariance matrix of the motion model $Q_t$**\n",
    "\n",
    "\n",
    "$$\n",
    "Q_t = \n",
    "\\begin{bmatrix}\n",
    "Var(x_t) & Cov(x_t, y_t) & Cov(x_t, \\gamma_t) \\\\\n",
    "Cov(y_t, x_t) & Var(y_t) & Cov(y_t, \\gamma_t) \\\\\n",
    "Cov(\\gamma_t, x_t) & Cov(\\gamma_t, y_t) & Var(\\gamma_t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To implement a Kalman filter, the variance of the motion model has te be derived. \n",
    "While the odometry system provides only the velocity of the two motors, the geometry of the robot can be used to calculate its position and orientation, hence: $$x=f_1(v_r,v_l) = \\frac{R}{2} (v_r + v_l) \\cos(\\gamma) \\Delta t$$ $$y=f_2(v_r,v_l) = \\frac{R}{2} (v_r + v_l) \\sin(\\gamma) \\Delta t$$ $$\\gamma=f_3(v_r,v_l) =\\frac{R}{L} (v_r-v_l) \\Delta t$$ where $v_r$ and $v_l$ are the velocity of the two motors, $R$ is the radius of the wheel and $L$ is the distance between the two wheels . The propagation error is then derived: $$Var(x) = (\\frac{\\partial f_1}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l})^2 Var(v_l) +2(\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) $$ $$Var(y) = (\\frac{\\partial f_2}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_2}{\\partial v_l})^2 Var(v_l) +2(\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) $$ $$Var(\\gamma) = (\\frac{\\partial f_3}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_3}{\\partial v_l})^2 Var(v_l) +2 (\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l)  $$ \n",
    "\n",
    "$v_r$ and $v_l$ are considered uncorrelated, so after deriving the partial derivatives the diagonal terms of $Q_t$ become:\n",
    "\n",
    "$$Var(x) =  (\\frac{R}{2} \\cos(\\gamma) \\Delta t)^2 (Var(v_r)+Var(v_l))$$ \n",
    "$$Var(y) = (\\frac{R}{2} \\sin(\\gamma) \\Delta t)^2 (Var(v_r)+Var(v_l))$$ \n",
    "$$Var(\\gamma) =  (\\frac{R}{L}\\Delta t)^2 (Var(v_r)+Var(v_l))$$\n",
    "\n",
    "The off-diagonal terms of $Q_t$ can then be calculated (taking in account the error propagation):\n",
    "$$ Cov(x,y) = Cov(y,x)  = (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_2}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_2}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_2}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_2}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "$$ Cov(x,\\gamma) = Cov(\\gamma,x) = (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "$$ Cov(y,\\gamma) = Cov(\\gamma,y) = (\\frac{\\partial f_2}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_2}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_2}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_2}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "\n",
    "After deriving the partial derivatives the following equations are obtained:\n",
    "\n",
    "$$ Cov(x,y) = \\frac{(R  \\Delta t)}{4}^2(\\cos(\\gamma)\\sin(\\gamma))(Var(v_r)+Var(v_l)) $$\n",
    "$$ Cov(x,\\gamma) = \\frac{(R \\Delta t)^2}{2L} \\cos(\\gamma) (Var(v_r)-Var(v_l) )$$\n",
    "$$ Cov(y,\\gamma) = \\frac{(R \\Delta t)^2}{2L} \\sin(\\gamma) (Var(v_r)-Var(v_l)) $$\n",
    "\n",
    "Now that the covariance matrix $Q_k$ is formulated, the parameters $Var(v_r)$ and $Var(v_l)$ can be obtained empirically. By collecting data from the Thymio proceeding at a constant speed of $200$ $Thymiounits$, the variance of the two motors were obtained (in $(rad/s)^2$): \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/motor_speed.png\" alt=\"Description of the image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "$$Var(v_r) = 0.013997244458988999 (rad/s)^2$$\n",
    "$$Var(v_l) = 0.007137898281767495 (rad/s)^2$$\n",
    "\n",
    "After some simulation for different values of $\\gamma$ from $0°$ to $180°$ it was found that the matrix $Q_t$ is diagonal.\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Q.png\" alt=\"Description of the image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "To ensure that the Kalman filter minimizes the variance of the error between the actual state and the predicted state, it is necessary to have a $Q$ matrix that is not time-varying ($Q_t = Q$). To achieve this, it is essential to find a $Q$ matrix that takes scalar inputs. Through simulation,  a matrix $Q$ was selected with values that maximize the variance, thereby obtaining a conservative estimate of the matrix.\n",
    "\n",
    "$$\n",
    "Q =\n",
    "\\begin{bmatrix}\n",
    "0.20000 & 0.00000 & 0.00000 \\\\\n",
    "0.00000 & 0.20000 & 0.00000 \\\\\n",
    "0.00000 & 0.00000 & 0.00070\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "Another crucial aspect to consider is that in the presented model, the input contribution ($ B_{t-1}u_{t-1}$) represents a deterministic quantity. As a result, the matrix $Q$ accounts solely for the state error, excluding any input error. A common practical approach in control theory is to incorporate the input error into an experimental variable, $\\alpha$, which requires experimental tuning. Consequently, the actual matrix $Q$ becomes $Q$ =$\\alpha  Q$, effectively encompassing the error associated with the input aswell. After some tuning it was found that a good matrix $Q$ for this application was:\n",
    "\n",
    "$$\n",
    "Q = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 \\\\\n",
    "0& 2 & 0 \\\\\n",
    "0 & 0 & 0.07\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deriving the observation model of the Thymio Robot**\n",
    "\n",
    "Now that a state space model that includes also the process noise has been derived, an observation model is required in order to be able to implement the Kalman filter. \n",
    "\n",
    "An observation model describes how the sensor outputs $y_t$ are related to the state vector $s_t$, considering also a vector of observation noise $ \\nu_{t} = [ \\nu^1_{t}, \\nu^2_{t}, \\nu^3_{t} ] ^T$\n",
    "with zero mean and covariance matrix $R$ (which is computed after some experimentation explained in the next section). \n",
    "The observation model is defined as follows: \n",
    "$$y_t = H_t s_t + \\nu_t$$ \n",
    "Since it is possible to detect the position and orientation of the Thyimio in the environment using a camera, the matrix $H$ becomes an identity matrix, hence the observation model can be defined as follows:\n",
    "\n",
    "$$\n",
    "y_{t} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\nu^1_{t}\\\\\n",
    "\\nu^2_{t}\\\\\n",
    "\\nu^3_{t}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It's important to notice that the system is robust even to the absence of camera measurements, even if the matrix $H$ is not changed. More details on this topic are provided in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calcultating the covariance matrix of the observation model $R_t$**\n",
    "\n",
    "The observation vector is denoted as follows:\n",
    "\n",
    "$$\n",
    "z_{t} =\n",
    "\\begin{bmatrix}\n",
    "x^o_{t} \\\\\n",
    "y^o_{t} \\\\\n",
    "\\gamma^o_{t}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "The variance of the observation will then be:\n",
    "\n",
    "$$\n",
    "R_t = \n",
    "\\begin{bmatrix}\n",
    "Var(x^o) & Cov(x^o, y^o) & Cov(x^o, \\gamma^o) \\\\\n",
    "Cov(y^o, x^o) & Var(y^o) & Cov(y^o, \\gamma^o) \\\\\n",
    "Cov(\\gamma^o, x^o) & Cov(\\gamma^o, y^o) & Var(\\gamma^o)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that those terms are different from the ones of the motion model because the observation noise is related to the camera and the computer vision setup and not to the Thymio. This matrix changes in relation to the visibility of the robot. Two different scenarios are considered: when the Thymio is visible and when it is not. Consequently $2$ matrices $R$ and $R_{nc}$ are defined. If the Thymio is not visible the incertaitnty grows to infinity, corresponding to the case using matrix $R_{nc}$ :\n",
    "$$\n",
    "R_{nc} = \n",
    "\\begin{bmatrix}\n",
    "\\infty &0 & 0 \\\\\n",
    "0 & \\infty & 0 \\\\\n",
    "0 & 0 & \\infty \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now it is possible to compute $R_t = R$ empirically. To do this, first a callibration grid in an A4 sheet was created, ensuring an accurate positioning of the Thymio robot before detecting its location and orientation. The A4 paper was generated using ChatGPT-4. The LLM generated the following file after the requirements were specified:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/square_design.png\" alt=\"Description of the image\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "After this, the required variance was computed mesuring the delta between the measure of the Thymio provided by the camera and its real position. The mesurment was repeated 10 times and the average values for $Var(x^o)$, $Var(y^o)$ and $Var(\\gamma^o)$ were calculated. To be consistent with the units of the motion model, the camera measurements have been converted from pixels to millimeters.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/R_exp.jpg\" alt=\"Description of the image\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$Var(x^o) = 0.7 (mm)^2$$\n",
    "$$Var(y^o) = 0.7 (mm)^2$$\n",
    "$$Var(\\gamma^o) = 0.0014 (rad)^2$$\n",
    "\n",
    "Hence the $R$ matrix is:\n",
    "$$\n",
    "R = \n",
    "\\begin{bmatrix}\n",
    "0.7 &0 & 0 \\\\\n",
    "0 & 0.7 & 0 \\\\\n",
    "0 & 0 & 0.0014\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementing the Extended Kalman Filter**\n",
    "\n",
    "Since the model is not linear (the $B$ matrix has a trigonometrical relation with the state varaiable $\\gamma$), the implementation of an Extended Kalman Filter is required.\n",
    "\n",
    "Due to the previous consideration, both the $R$ and the $R_{nc}$ are in place to take in to account when the Thymio is visible and when it is not.\n",
    "\n",
    "For the implementation, the measurement of the position and orientation of the Thymio in the environment obtained using the camera is refered as \n",
    "\n",
    "$$\n",
    "z_{t} =\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Robot control**\n",
    "\n",
    "Initially, a simple P controller was employed for the control law, aiming to minimize the perpendicular distance from the robot to the ideal trajectory line. However, this controller led to instability issues. Consequently, the decision was made to switch to a proportional-integral (PI) controller for the angle and a proportional (P) controller for the distance to the optimal path, combined. A differential speed is applied to the wheels of the robot based on the output of the controller.\n",
    "\n",
    "This controller functions based on the angle error between the desired angle and the current angle of the Thymio robot, along with the perpendicular distance from the Thymio to the line it should be following. Two main modes govern the controller's operation: alignment mode and forward mode.\n",
    "\n",
    "Alignment Mode: In this mode, the controller exclusively addresses the angle error. It is utilized when the robot reaches each point in the path, preparing for the next segment.\n",
    "\n",
    "\n",
    "Forward Mode: In this mode, the controller considers both the angle error and the perpendicular distance to the line. It becomes active when the robot is in motion between two points in the path.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/control.png\" alt=\"Control error diagram\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main**\n",
    "\n",
    "The main function is the core of the project. It is responsible for the initialization of the different modules and the communication between them.\n",
    "\n",
    "The function follows the concept of operations presented in the figure below:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/conOps.png\" alt=\"Cocept of operations\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from src.computerVision import correctPerspectiveStream, findCorners, getPerspectiveMatrix, findThymio, findGlobalObstacles, findGoal\n",
    "from src.kalman import estimatePosition, inverseSpeedConversion, r11, r22, r33, Cl, Cr, R, L\n",
    "from src.pathPlanning import buildGraph\n",
    "from src.robotControl import robotController, checkForObstacles\n",
    "\n",
    "XYMIRROR = False\n",
    "IMAGE_WIDTH = 1920\n",
    "IMAGE_HEIGHT = 1080\n",
    "POSITION_THRESHOLD = 50\n",
    "KIDNAPPING_THRESHOLD = 100\n",
    "KIDNAPPING_TIME = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()\n",
    "\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def motors(l_speed=500, r_speed=500, verbose=False):\n",
    "\n",
    "    global motor_left_target, motor_right_target\n",
    "\n",
    "    l_speed = int(l_speed)\n",
    "    r_speed = int(r_speed)\n",
    "    \n",
    "    motor_left_target = l_speed\n",
    "    motor_right_target = r_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "@tdmclient.notebook.sync_var \n",
    "def horiz_sensor():\n",
    "    global prox_horizontal\n",
    "    return prox_horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "@tdmclient.notebook.sync_var \n",
    "def getVerticalDistance():\n",
    "    global prox_ground_reflected\n",
    "    return prox_ground_reflected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_python\n",
    "\n",
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "nf_leds_prox_h(0,0,0,0,0,0,0,0) \n",
    "nf_leds_rc(0)\n",
    "nf_leds_temperature(0, 0)\n",
    "nf_leds_bottom_right(0,0,0)\n",
    "nf_leds_bottom_left(0,0,0)\n",
    "nf_leds_top(0,0,0)\n",
    "nf_leds_prox_v(0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: these functions are defined here and not in their respective files because they call the functions that interact with the robot defined above\n",
    "\n",
    "def checkForKidnap():\n",
    "    ver=getVerticalDistance()\n",
    "    if ver[0] < KIDNAPPING_THRESHOLD or ver[1] < KIDNAPPING_THRESHOLD:\n",
    "        print(\"Kidnapped!\")\n",
    "        motors(0,0)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def avoidObstacle(leftOrRight, initTime, currentTime):\n",
    "    motors(0,0)\n",
    "    \n",
    "    tTurn = 1\n",
    "    tStraight = 2\n",
    "    vStraight = 200\n",
    "\n",
    "    if leftOrRight == 'right':\n",
    "        l=-200\n",
    "        r=200\n",
    "    elif leftOrRight == 'left':\n",
    "        l=200\n",
    "        r=-200\n",
    "\n",
    "    motors(l_speed=l, r_speed=r)\n",
    "    if currentTime - initTime < tTurn:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=vStraight, r_speed=vStraight)\n",
    "    if currentTime - initTime < tTurn + tStraight:\n",
    "        return True\n",
    "    \n",
    "\n",
    "    motors(l_speed=-l, r_speed=-r)\n",
    "    if currentTime - initTime < 2*tTurn + tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=vStraight, r_speed=vStraight)\n",
    "    if currentTime - initTime < 2*tTurn + 2.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=-l, r_speed=-r)\n",
    "    if currentTime - initTime < 3*tTurn + 2.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=vStraight, r_speed=vStraight)\n",
    "    if currentTime - initTime < 3*tTurn + 3.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=l, r_speed=r)\n",
    "    if currentTime - initTime < 4*tTurn + 3.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=0, r_speed=0)\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "previousTime = time.time()\n",
    "\n",
    "#state control variables\n",
    "correctedCam = False\n",
    "environmentSetup = False\n",
    "redoPath = False\n",
    "avoidingObstacle = False\n",
    "wasKidnapped = False\n",
    "aligned = False\n",
    "\n",
    "#initializing variables\n",
    "path = []\n",
    "goal = np.array([0,0])\n",
    "lSpeed = 0\n",
    "rSpeed = 0\n",
    "P_k = np.array([[r11,0,0],[0,r22,0],[0,0,r33]])\n",
    "postionHistory = []\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    key = cv2.waitKey(1)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: failed to capture image\")\n",
    "        break\n",
    "\n",
    "    if not correctedCam:\n",
    "        # Map the image to the corner\n",
    "        position, angle, _=findThymio(frame)\n",
    "        estimateState = np.array([position[0],IMAGE_HEIGHT-position[1],angle])\n",
    "        cv2.imshow('Thymio Camera', frame)\n",
    "        try:\n",
    "            centroids = findCorners(frame)\n",
    "            perspectiveMatrix = getPerspectiveMatrix(centroids)\n",
    "            correctedCam = True\n",
    "        except:\n",
    "            print(\"Fail\")\n",
    "            continue\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Correct the perspective of the image\n",
    "    if correctedCam:\n",
    "        frame = correctPerspectiveStream(frame,perspectiveMatrix)\n",
    "        frameToPlot = frame\n",
    "\n",
    "\n",
    "\n",
    "    current_time = time.time()\n",
    "    dt = current_time - previousTime\n",
    "    if dt < 0.15:\n",
    "        time.sleep(0.15-dt)\n",
    "\n",
    "    previousTime = current_time\n",
    "\n",
    "\n",
    "    previousControlVector = inverseSpeedConversion(rSpeed,lSpeed,R,L,Cr,Cl)\n",
    "\n",
    "    position, angle, estimateState, P_k = estimatePosition(frame,previousControlVector,dt,P_k,estimateState)\n",
    "\n",
    "    postionHistory.append(position)\n",
    "\n",
    "\n",
    "\n",
    "    #plot the points in position history in black\n",
    "    for i in range(len(postionHistory)-1):\n",
    "        cv2.circle(frameToPlot, (int(postionHistory[i][0]), int(IMAGE_HEIGHT-postionHistory[i][1])), 5, (0, 0, 0), -1)\n",
    "\n",
    "    #plot the goal\n",
    "    cv2.circle(frameToPlot, (int(goal[0]), int(goal[1])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    #plot the path lines\n",
    "    for i in range(len(path)-1):\n",
    "        cv2.line(frameToPlot,(int(path[i][0]),int(IMAGE_HEIGHT-path[i][1])),(int(path[i+1][0]),int(IMAGE_HEIGHT-path[i+1][1])),(255,255,255),2)\n",
    "        cv2.circle(frameToPlot, (int(path[i][0]), int(IMAGE_HEIGHT-path[i][1])), 5, (255, 255, 255), -1)\n",
    "        \n",
    "    cv2.imshow('Thymio Camera', frameToPlot)\n",
    "    \n",
    "    \n",
    "    if checkForKidnap():\n",
    "        #cambiare le condizioni\n",
    "        print(\"kidnapped\")\n",
    "        lSpeed = 0\n",
    "        rSpeed = 0\n",
    "        wasKidnapped = True\n",
    "        continue\n",
    "\n",
    "\n",
    "    if wasKidnapped:\n",
    "        time.sleep(KIDNAPPING_TIME)\n",
    "        wasKidnapped = False\n",
    "        redoPath = True\n",
    "        continue\n",
    "\n",
    "\n",
    "    if not avoidingObstacle:\n",
    "        leftOrRight = checkForObstacles(horiz_sensor())\n",
    "\n",
    "\n",
    "    if leftOrRight != None or avoidingObstacle:\n",
    "        if not avoidingObstacle:\n",
    "            initTime = time.time()\n",
    "        avoidingObstacle = avoidObstacle(leftOrRight, initTime, time.time())\n",
    "        redoPath = True\n",
    "        continue\n",
    "    \n",
    "\n",
    "    if (key == ord(' ') and environmentSetup == False) or redoPath:\n",
    "        # Capture an image\n",
    "        imagePath = 'capturedImage.jpg'\n",
    "        cv2.imwrite(imagePath, frame)\n",
    "\n",
    "        capturedImage = cv2.imread(imagePath)\n",
    "        position, angle, estimateState, P_k = estimatePosition(capturedImage,previousControlVector,dt,P_k,estimateState)\n",
    "\n",
    "        greenObjects = findGlobalObstacles(capturedImage)\n",
    "\n",
    "        goal = findGoal(capturedImage)\n",
    "        \n",
    "        print(goal)\n",
    "\n",
    "        cv2.imshow('Thymio Camera', capturedImage)\n",
    "        \n",
    "\n",
    "        path=buildGraph(greenObjects,position,goal)\n",
    "        pointCount = 0\n",
    "        print(path)\n",
    "\n",
    "        if redoPath:\n",
    "            redoPath = False\n",
    "            continue\n",
    "        environmentSetup = True\n",
    "        \n",
    "        #cv2.waitKey(0)  # Wait until any key is pressed to close the image window\n",
    "\n",
    "\n",
    "\n",
    "    if environmentSetup:\n",
    "\n",
    "        if np.linalg.norm(position - path[pointCount]) < POSITION_THRESHOLD:\n",
    "            aligned = False\n",
    "            pointCount += 1\n",
    "            motors(0,0)\n",
    "\n",
    "            if pointCount == len(path):\n",
    "                motors(0,0)\n",
    "                print(\"Goal reached\")\n",
    "                break\n",
    "\n",
    "            continue\n",
    "        \n",
    "        if not aligned:\n",
    "            lSpeed,rSpeed,distanceError,angleError = robotController(path[pointCount-1],path[pointCount],position,angle,dt,alignMode=True)\n",
    "            motors(lSpeed,rSpeed)\n",
    "            if np.abs(angleError) < 0.1:\n",
    "                aligned = True\n",
    "                motors(0,0)\n",
    "            continue\n",
    "\n",
    "\n",
    "        lSpeed,rSpeed,distanceError,angleError = robotController(path[pointCount-1],path[pointCount],position,angle,dt,alignMode=False)\n",
    "        motors(lSpeed,rSpeed)\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motors(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sources**\n",
    "\n",
    "Course \"Basics of Mobile Robotics\" by Prof. Francesco Mondada (MICRO-452): https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "Automatic Addison: https://automaticaddison.com\n",
    "\n",
    "mouhknowsbest (Youtube Channel): https://www.youtube.com/watch?v=aE7RQNhwnPQ\n",
    "\n",
    "Chat-GPT4: https://chat.openai.com/\n",
    "\n",
    "Copilot: https://copilot.github.com/\n",
    "\n",
    "Numpy: https://numpy.org/doc/\n",
    "\n",
    "OpenCV: https://docs.opencv.org/\n",
    "\n",
    "Pyvisgraph: https://github.com/TaipanRex/pyvisgraph/tree/master\n",
    "\n",
    "matplotlib: https://matplotlib.org/stable/contents.html\n",
    "\n",
    "TDM Client: https://github.com/epfl-mobots/tdm-python/tree/main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile_robotics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
