{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basics of Mobile Robotics Project** \n",
    "## **EPFL - December 2023**\n",
    "### **Professor: Francesco Mondada**\n",
    "### **Authors:**\n",
    "- Evangelista Santiago Roberto\n",
    "\n",
    "- Giovine Angelo\n",
    "\n",
    "- He Weifeng\n",
    "\n",
    "- Syla Valmir\n",
    "\n",
    "## **Tables of contents**\n",
    "\n",
    "- [Demo Videos](#Demo-Videos)\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "  - [Project description and details](#Project-description-and-details)\n",
    "  - [Modules and libraries required](#Modules-and-libraries-required)\n",
    "\n",
    "- [Environment](#Environment)\n",
    "\n",
    "- [Vision](#Vision)\n",
    "  - [Camera](#Camera)\n",
    "  - [Environment mapping](#Environment-mapping)\n",
    "  - [Global obstacle detection](#Global-obstacle-detection)\n",
    "  - [Robot detection](#Robot-detection)\n",
    "  - [Goal detection](#Goal-detection)\n",
    "\n",
    "  \n",
    "\n",
    "- [Global Navigation](#Global-Navigation)\n",
    "  - [Visibility graph](#Visibility-graph)\n",
    "\n",
    "\n",
    "\n",
    "- [Local Navigation](#Local-Navigation)\n",
    "  - [Local obstacle detection](#Local-obstacle-detection)\n",
    "  - [Kidnaping](#Kidnaping)\n",
    "\n",
    "- [Filtering](#Filtering:-Extended-Kalman-Filter)\n",
    "  - [Why Kalman?](#Deriving-the-state-space-model-of-the-Thymio-Robot)\n",
    "  - [Deriving the state space model of the Thymio Robot](#Deriving-the-state-space-model-of-the-Thymio-Robot)\n",
    "  - [Dealing with the input](#Dealing-with-the-input)\n",
    "  - [Experimentation to derive $Cl$ and $Cr$](#Experimentation-to-derive-$Cl$-and-$Cr$)\n",
    "  - [Calcultating the covariance matrix of the motion model $Q_t$](#Calcultating-the-covariance-matrix-of-the-motion-model-$Q_t$)\n",
    "  - [Deriving the observation model of the Thymio Robot](#Deriving-the-observation-model-of-the-Thymio-Robot)\n",
    "  - [Calcultating the covariance matrix of the observation model $R_t$ ](#Calcultating-the-covariance-matrix-of-the-observation-model-$R_t$ )\n",
    "  - [testImplementing the Extended Kalman Filter](#Implementing-the-Extended-Kalman-Filter)\n",
    "\n",
    "\n",
    "\n",
    "- [Control law ](#Control-law)\n",
    "\n",
    "- [Main](#Main)\n",
    "\n",
    "- [The Code](#The-Code)\n",
    "\n",
    "- [Sources](#Sources)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Demo videos**\n",
    "\n",
    "The optimal path is displayed in white, while the real-time estimated position of the Thymio, obtained through filtering, is indicated by a black dotted line.\n",
    "\n",
    "Full demo + Normal situation + Kidnapping + Camera obstructed + Local obstacles: https://youtu.be/rz_EOHwGMOw\n",
    "\n",
    "Full demo: https://youtu.be/DTvLO2GhbcM\n",
    "    \n",
    "Normal situation: https://youtu.be/M8M3lAdUMPQ\n",
    "\n",
    "Kidnapping: https://youtu.be/5M4zRgAHsiA\n",
    "\n",
    "Camera obstructed: https://youtu.be/vcZKryRRfx0\n",
    "\n",
    "Local obstacles: https://youtu.be/Q0z2UTtgOtY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "#### **Project description and details:**\n",
    "The project is structured around five distinct modules, each tailored to a specific aspect of the Thymio Robot's navigation and operational capabilities:\n",
    "- Vision Module\n",
    "- Global Navigation Module\n",
    "- Local Navigation Module\n",
    "- Filtering Module\n",
    "- Robot control\n",
    "\n",
    "The project uses mainly the camelCase naming convention for variables and functions (everything except the kalman filter). For the kalman filter, the snake case convention was used. This was because of the ditribution of the work among the team members.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modules and libraries required:**\n",
    "\n",
    "The following modules and libraries are required to run the code:\n",
    "- numpy\n",
    "- matplotlib\n",
    "- cv2\n",
    "- math\n",
    "- time\n",
    "- pyvisgraph\n",
    "- tdmclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment**\n",
    "\n",
    "For the chosen environment, we have selected a monochromatic blue background, complemented by 2D green obstacles and a yellow target area. The Thymio robot is identified by a distinctive red \"mask\" applied to it. The environment was constructed on a pool table.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/environment.jpeg\" alt=\"Environment on the pool table\" width=\"400\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/thymio.jpeg\" alt=\"Thymio with the mentioned mask\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "In the development of our Mobile Robotics Project, we employed a forward-thinking strategy by incorporating ChatGPT-4's capabilities. Our choice was influenced by ChatGPT-4's sophisticated computational prowess and its adeptness in crafting intricate geometric figures. We primarily utilized the model to construct a variety of polygons, which functioned as fixed obstacles on our map. We observed that while the model could produce a broad spectrum of shapes, from simple triangles to elaborate polygons, it encountered difficulties in creating a PDF with non-overlapping shapes. Additionally, it was challenged in randomly generating polygons, often resulting in blank outputs or error messages. Consequently, we intervened by specifying the dimensions, forms, and hues of the polygons. As a result, our map was not entirely AI-generated but rather a collaborative effort, blending our specific polygon choices with GPT-4's assembly skills. This experience once again underscored the limitations of a Large Language Model (LLM) in tasks demanding significant creativity.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/GPT_error.png\" alt=\"GPT error message\" width=\"450\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/GPT_env.png\" alt=\"polygons generated by GPT4 \" width=\"550\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Camera**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/iPhone.jpg\" alt=\"very bad image of the concept\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "The camera used in the project was not the USB camera provided by the course. We started the development with the USB camera, but while experimenting with the camer choice we discovered that using a macbook, the camera of the iphone with the same icloud account is automatically recognized as a webcam wirelessly. This was great considering the camera had to be placed in the roof pointing down at the table. This allowed us to stick the phone to the roof and not have any cables hanging from the roof to the computer. From python, the camera is recognized as a normal webcam, so the code did not have to be changed.\n",
    "\n",
    "#### **Environment mapping**\n",
    "\n",
    "The computer vision module maps the environment to a 2d plane, to allow for correct functioning of the following modules. To do this, four small green markers are placed on the corners of the table. The camera detects the markers (the global obstacles are also green so the external 4 green detected objects are taken) and maps these point to certain point on the plane depending on the aspect ratio of the markers.\n",
    "\n",
    "### **Global obstacle detection**\n",
    "\n",
    "The global obstacle detection is done by detecting the green objects in the environment. These objects are filtered by size and are expanded since the global navigation algorithm used is the visibility graph.\n",
    "\n",
    "### **Robot detection**\n",
    "\n",
    "The robot is found by detecting the red mask on the robot. The mask is a composed of a white background and two red rectangles, one large in the back (center between the two wheels, important since this is the center of rotation) and one small in the front. Two rectangles are used to be able to discriminate the angle from 0 to 2pi instead of 0 to pi. This method gives us the robot's position and orientation.\n",
    "\n",
    "### **Goal detection**\n",
    "The goal is detected using a yellow detector. The centroid of the largest yellow object is taken as the goal position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Global Navigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visibility graph**\n",
    "For the global navigation, we used the visibility graph algorithm. To do this we used the library pyvisgraph. The library has all the functions needed including the finding of the shortest path, so no extra libraries (apart from the already used numpy and matplotlib) were needed. It is important to note that the library uses a different coordinate standard than openCV, so an extra function was needed to convert the coordinates. Another important addition was the pushing far of the point in the edge. When initially testing the system we encountered issues when a global obstacle was close to the edge even if expanded for the robot size. To resolve this we pushed the points of the global obstacles that are on the edge to a distance far away from the center. This ensured that the path planning algorithm would not consider them.\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/pointsInPath.png\" alt=\"Plot with points in the shortest path\" width=\"400\" style=\"margin-right: 10px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Local Navigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Local obstacle detection**\n",
    "\n",
    "To detect the local obstacles we used the horizontal distance sensors in the front of the robot. Depending on in which side the obstacle is detected, the robot will turn in the opposite direction and avoid the obstacle using a squared path manouver. The moving of the robot was done using a direct speed input and timers, since it proved to be simple but effective. The values of the timers were found by trial and error. Even if considerably precise, at the end of the manouver and once the obstacle has been avoided, the global path planning algorithm is run again to ensure the continuation is smooth and the robot recovers in th ebest possible manner.  This method has some limitations though. The aproximated size of the obstacles has to be known a priori and sufficient place to do the manouver has to be available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kidnaping**\n",
    "\n",
    "\n",
    "To detect the kidnaping of the robot we use the vertical distance sensors on the robot. If the robot is lifted from the table a kidnaped flag is activated and while the robot is not positioned on the tale the robot turns off the motors and waits. When the robot is placed back in the table, a few seconds are waited until the robot is stable and no other perturbations are present (hands that can confuse the robot location for example). One this is the case, the global path planning algorithm is run again to find the new path to the goal. \n",
    "\n",
    "We used the vertical distance sensors in the ground reflected mode. After experimentation we remarked that this was the mode that showed the most difference when the robot was placed on the table or lifted from it, facilitating the determination of the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Filtering: Extended Kalman Filter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Kalman?**\n",
    "\n",
    "During the course we have encountered different Bayes filter to estimate the state of a robot in a given environment. For example, we could have used a particle filter, but it is more computationally expensive and it is not necessary for this application. For our task we have choosen the Kalman filter, a recursive filter that estimates the state of a system from a series of noisy measurements. It is a very efficient algorithm that can be used to solve the localization problem of a robot, but has a crucial assumption about the distribution of the noise. In fact, it assumes that the noise is Gaussian distributed, which is not always the case in real life. However, in differential robots, like the Thymio, odometry is primarily based on the velocity sensors of the motors to calculate the robot's position and orientation ($x, y, \\gamma$). The errors in odometry can arise from various sources:\n",
    "- Sensor Measurement Errors: The sensors might not be perfectly accurate or might exhibit some drift over time.\n",
    "- Mechanical Misalignments: Slight imperfections in the robot's construction or wear in components can cause discrepancies.\n",
    "- Differential Motion Transmission Variances: Factors like friction, tire pressure, or differences in the movement surface can affect how motion is transmitted from the motors to the wheels.\n",
    "- Slippage Error: The wheels may slip or lose traction, leading to inaccurate distance measurements.\n",
    "\n",
    "Since each of these error sources contributes small, independent increments to the overall odometry error. According to the Central Limit Theorem, when a large number of small, independent effects are summed, their overall distribution tends to approach a Gaussian distribution. This implies that even if each individual source of error might not be Gaussian, their cumulative effect tends towards a normal distribution. \n",
    "\n",
    "Since the odometry noise can be assumed to be Gaussian distributed, the Kalman filter, that's easy to implement, is a good choice for our application.\n",
    "\n",
    "Once again we have decied to collaborate with LLM. This time we wanted to generate an image suitable for this paragraph, but even if we were precise about the contest ChatGPT-4 was able to produce an image not appropriated for this paragraph, the image is not scientific or accurated but we decided to use it anyway, to highlight another limit of LLM.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/GPT_Kalman.png\" alt=\"very bad image of the concept\" width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deriving the state space model of the Thymio Robot**\n",
    "\n",
    "Source: https://automaticaddison.com/how-to-derive-the-state-space-model-for-a-mobile-robot/ \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/state_space_.png\" alt=\"2D image of a differential drive robot\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "First of all, we need to define the state and input variables. We will use the following state variables: $x$ and $y$ for the position, and $\\gamma$ for the orientation, since we can control our Thyimio in speed, we can define our input variables $v$ forward velocity, and $\\omega$ angular velocity . After some simple trigonometric analysis is easy to define the state vector $s$ is defined as follows: \n",
    "$$\n",
    "s_{t} =\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{t-1} + v_{t-1}\\cos\\gamma_{t-1} \\cdot \\Delta t \\\\\n",
    "y_{t-1} + v_{t-1}\\sin\\gamma_{t-1} \\cdot \\Delta t \\\\\n",
    "\\gamma_{t-1} + \\omega_{t-1} \\cdot \\Delta t\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "f_1 \\\\\n",
    "f_2 \\\\\n",
    "f_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence our input vector $u$ is defined as follows:\n",
    "\n",
    "$$\n",
    "u_{t} = \n",
    "\\begin{bmatrix}\n",
    "    v_{t} \\\\ \n",
    "     \\omega_{t}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We would now define our state space model as our system is linaer so in this form:  $s_{t}= A_{t-1}s_{t-1} + B_{t-1}u_{t-1}$, where $A_{t-1}$ is the state matrix and $B_{t-1}$ is the input matrix. But our original system is not linear. However, we can approximate our system by using the Jacobian matrix.\n",
    "Our Thymio moves only when it receives instructions to rotate its wheels. As a result, in this scenario, the $A_{t-1}$ matrix is an identity matrix (linear relation). For the $B_{t-1}$ matrix we can use the Jacobian matrix, which is defined as follows:\n",
    "\n",
    "$$\n",
    "B_{t-1} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial v_{t-1}} & \\frac{\\partial f_1}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_2}{\\partial v_{t-1}} & \\frac{\\partial f_2}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_3}{\\partial v_{t-1}} & \\frac{\\partial f_3}{\\partial \\omega_{t-1}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "\\sin(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "0 & \\Delta t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "If we consider also the process noise $ w_{t} = [ w^1_{t}, w^2_{t}, w^3_{t} ] $\n",
    " , we can define our state space model as follows:\n",
    "$$s_{t}= A_{t-1}s_{t-1} + B_{t-1}u_{t-1} +w_{t-1}$$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{t-1} \\\\\n",
    "y_{t-1} \\\\\n",
    "\\gamma_{t-1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "\\sin(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "0 & \\Delta t\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{t-1} \\\\\n",
    "\\omega_{t-1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w^1_{t-1} \\\\\n",
    "w^2_{t-1} \\\\\n",
    "w^3_{t-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that we are assuming that the process noise is zero-mean Gaussian noise with covariance matrix $Q_k$, which is computed after some experimentation explained in the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dealing with the inputs**\n",
    "\n",
    "Source: https://www.youtube.com/watch?v=aE7RQNhwnPQ\n",
    "\n",
    "If we notice, our state space model is controlled in $v$ forward velocity, and $\\omega$ angular velocity but in reality we can only control the speed of the two motors $v_{right}$, and $v_{left}$ of our Thymio. So we need to find a way to convert the speed of the robot $v$ and $\\omega$ into $v_{right}$, and $v_{left}$. We can do this by using the following equations:\n",
    "\n",
    "$v_{right} = \\frac{2v+\\omega L}{2R}$ , $v_{left} = \\frac{2v-\\omega L}{2R}$\n",
    "\n",
    "Where $L$ is the distance between the two wheels and $R$ is the radius of the wheels.\n",
    "\n",
    "In order to control the Thymio we must convert the speed of the two motors from the Thymio unit system to $v_{right}$, and $v_{left}$  we can do this buy using the convertion factor $Cr$ and $Cl$ calculated through experimentation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experimentation to derive $Cl$ and $Cr$**\n",
    "\n",
    "In order to use the $Cl$ and $Cr$ values in the program, we need to derive them from the experimental data. This section contains the collected data and the results. First we have marked each wheel of the Thymio robot then we have run the robot at a fixed speed $spd = 200$  $Thymio units$ and measured the number of revolution of each wheel in a fixed time interval $t$. We have repeated this experiment 10 times and calculated our average values for $Cl$ and $Cr$.\n",
    "Since we have noticed that $Cl$ and $Cr$ are not proportional with the Thymio motors speed, we have decided to use the speed used to derive this coefficent as the constant speed input in our controller.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Thymio_wheel.jpeg\" alt=\"Description of Thymio wheel\" width=\"500\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/Cl_Cr.png\" alt=\"Description of second image\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "$$Cl = 69.33821285 \\frac{s}{rad}Thymio units,Cr = 70.74580573 \\frac{s}{rad}Thymio units$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calcultating the covariance matrix of the motion model $Q_t$**\n",
    "\n",
    "\n",
    "$$\n",
    "Q_t = \n",
    "\\begin{bmatrix}\n",
    "Var(x_t) & Cov(x_t, y_t) & Cov(x_t, \\gamma_t) \\\\\n",
    "Cov(y_t, x_t) & Var(y_t) & Cov(y_t, \\gamma_t) \\\\\n",
    "Cov(\\gamma_t, x_t) & Cov(\\gamma_t, y_t) & Var(\\gamma_t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To implement a Kalman filter, we first need to know the variance of the motion model. \n",
    "While my odometry system provides only the velocity of the two motors, we can use the geometry of the Thymio to calculate the position and orientation of the robot, hence: $$x=f_1(v_r,v_l) = \\frac{R}{2} (v_r + v_l) \\cos(\\gamma) \\Delta t$$ $$y=f_2(v_r,v_l) = \\frac{R}{2} (v_r + v_l) \\sin(\\gamma) \\Delta t$$ $$\\gamma=f_3(v_r,v_l) =\\frac{R}{L} (v_r-v_l) \\Delta t$$ where $v_r$ and $v_l$ are the velocity of the two motors, $R$ is the radius of the wheel and $L$ is the distance between the two wheels . Now we have to take in account the error propagation, so we can calculate $$Var(x) = (\\frac{\\partial f_1}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l})^2 Var(v_l) +2(\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) $$ $$Var(y) = (\\frac{\\partial f_2}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_2}{\\partial v_l})^2 Var(v_l) +2(\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) $$ $$Var(\\gamma) = (\\frac{\\partial f_3}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_3}{\\partial v_l})^2 Var(v_l) +2 (\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l)  $$ \n",
    "\n",
    "We can consider $v_r$ and $v_l$ uncorrelated so after deriving the partial derivatives we can calculate the diagonal terms of $Q_t$ as follows:\n",
    "\n",
    "$$Var(x) =  (\\frac{R}{2} \\cos(\\gamma) \\Delta t)^2 (Var(v_r)+Var(v_l))$$ \n",
    "$$Var(y) = (\\frac{R}{2} \\sin(\\gamma) \\Delta t)^2 (Var(v_r)+Var(v_l))$$ \n",
    "$$Var(\\gamma) =  (\\frac{R}{L}\\Delta t)^2 (Var(v_r)+Var(v_l))$$\n",
    "\n",
    "Next we need to calculate the off-diagonal terms of $Q_t$, we can do it again by taking in account the error propagation:\n",
    "$$ Cov(x,y) = Cov(y,x)  = (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_2}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_2}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_2}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_2}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "$$ Cov(x,\\gamma) = Cov(\\gamma,x) = (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "$$ Cov(y,\\gamma) = Cov(\\gamma,y) = (\\frac{\\partial f_2}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_2}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_2}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_2}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "\n",
    "After deriving the partial derivatives we obatain:\n",
    "\n",
    "$$ Cov(x,y) = \\frac{(R  \\Delta t)}{4}^2(\\cos(\\gamma)\\sin(\\gamma))(Var(v_r)+Var(v_l)) $$\n",
    "$$ Cov(x,\\gamma) = \\frac{(R \\Delta t)^2}{2L} \\cos(\\gamma) (Var(v_r)-Var(v_l) )$$\n",
    "$$ Cov(y,\\gamma) = \\frac{(R \\Delta t)^2}{2L} \\sin(\\gamma) (Var(v_r)-Var(v_l)) $$\n",
    "\n",
    "Now that we have our covariance matrix $Q_k$ formulation, we can calulate the parameters $Var(v_r)$ and $Var(v_l)$ by experimentation. By simply collecting data from our Thymio proceding at a constant speed of $200$ $Thymiounits$, we have obtained the variance of the two motors, in $(rad/s)^2$ \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/motor_speed.png\" alt=\"Description of the image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "$$Var(v_r) = 0.013997244458988999 (rad/s)^2$$\n",
    "$$Var(v_l) = 0.007137898281767495 (rad/s)^2$$\n",
    "\n",
    "After some simulation for different values of $\\gamma$ from $0°$ to $180°$ we have found that the matrix $Q_t$ is diagonal.\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Q.png\" alt=\"Description of the image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "To ensure that the Kalman filter minimizes the variance of the error between the actual state and the predicted state, it is necessary to have a $Q$ matrix that is not time-varying ($Q_t = Q$). To achieve this, it is essential to find a $Q$ matrix that takes scalar inputs. Through simulation, we have selected a $Q$ matrix with values that maximize the variance values, thereby obtaining a conservative estimate of the matrix.\n",
    "\n",
    "$$\n",
    "Q =\n",
    "\\begin{bmatrix}\n",
    "0.20000 & 0.00000 & 0.00000 \\\\\n",
    "0.00000 & 0.20000 & 0.00000 \\\\\n",
    "0.00000 & 0.00000 & 0.00070\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "Another crucial aspect to consider is that in our model the input contribution ($ B_{t-1}u_{t-1}$) represents a deterministic quantity. As a result, our current $Q$ matrix accounts solely for the state error, excluding any input error. A common practical approach in control theory is to incorporate the input error into an experimental variable, $\\alpha$, which requires experimental tuning. Consequently, our actual $Q$ matrix becomes $Q$ =$\\alpha  Q$, effectively encompassing the error associated with the input as well. After some tuning we have found that a good matrix $Q$ for our application is:\n",
    "\n",
    "$$\n",
    "Q = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0. \\\\\n",
    "0& 2 & 0 \\\\\n",
    "0 & 0 & 0.07\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deriving the observation model of the Thymio Robot**\n",
    "\n",
    "Source: https://automaticaddison.com/how-to-derive-the-observation-model-for-a-mobile-robot/\n",
    "\n",
    "Now that we have a linear state space model of our Thyimio that includes also the process noise, we need an observation model in order to be able to implement the Kalman filter. \n",
    "\n",
    "An observation model describes how expected sensor outputs $y_t$ are related to the state vector $s_t$, considering also a vector of observation noise $ \\nu_{t} = [ \\nu^1_{t}, \\nu^2_{t}, \\nu^3_{t} ] ^T$\n",
    "with zero mean and covariance matrix $R$ (which is computed after some experimentation explained in the next section). \n",
    "The observation model is defined as follows: \n",
    "$$y_t = H_t s_t + \\nu_t$$ \n",
    "Since we are able to detect the position and the orientation of the Thyimio in the environment using camera, our matrix $H$ is the identity, hence we can define our observation model as follows:\n",
    "\n",
    "$$\n",
    "y_{t} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\nu^1_{t}\\\\\n",
    "\\nu^2_{t}\\\\\n",
    "\\nu^3_{t}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calcultating the covariance matrix of the observation model $R_t$**\n",
    "\n",
    "The matrix $R_t$ is obtain by experimentation, and contain the variance of the observation noise, in our case the variance of the position and orientation of the Thymio in the environment, we can denote the observation vector as follows:\n",
    "\n",
    "$$\n",
    "z_{t} =\n",
    "\\begin{bmatrix}\n",
    "x^o_{t} \\\\\n",
    "y^o_{t} \\\\\n",
    "\\gamma^o_{t}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We can calculate the variance of the observation and it will be:\n",
    "\n",
    "$$\n",
    "R_t = \n",
    "\\begin{bmatrix}\n",
    "Var(x^o) & Cov(x^o, y^o) & Cov(x^o, \\gamma^o) \\\\\n",
    "Cov(y^o, x^o) & Var(y^o) & Cov(y^o, \\gamma^o) \\\\\n",
    "Cov(\\gamma^o, x^o) & Cov(\\gamma^o, y^o) & Var(\\gamma^o)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that those term are different from the ones of the motion model, because the observation noise is releted with our camera and our image vision approach, not with the Thymio itself. This matrix changes in relation with the visibility of our Thymio, for simplicity we can assume two different scenarios, when the Thymio is visible and when it is not, so we can define just 2 matrix $R$ and $R_{nc}$.If we can not detect it the incertence grows to infinity, so we can define a matrix $R_{nc}$, in this case we set the variance of the observation noise to infinity:\n",
    "$$\n",
    "R_{nc} = \n",
    "\\begin{bmatrix}\n",
    "\\infty &0 & 0 \\\\\n",
    "0 & \\infty & 0 \\\\\n",
    "0 & 0 & \\infty \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now I can compute $R_t = R$ through experimentation, first we need to create an A4 paper as a calibration grid, ensuring the most accurate positioning of the Thymio robot before detecting its location and orientation. The A4 paper was easly generated using ChatGPT-4. The LLM after getting as input the shapes and the dimension needed for the grid was able to generate the following file:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/square_design.png\" alt=\"Description of the image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Then we have computed the required variance mesuring the delta between the measure of the Thymio provided by the camera and the real position of the Thymio. We have repeated this 10 times and calculated our average values for $Var(x^o)$, $Var(y^o)$ and $Var(\\gamma^o)$. To be consistent with the units of the motion model, we have converted the camera measure from pixels to millimiters.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/R_exp.jpg\" alt=\"Description of the image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$Var(x^o) = 0.7 (mm)^2$$\n",
    "$$Var(y^o) = 0.7 (mm)^2$$\n",
    "$$Var(\\gamma^o) = 0.0014 (rad)^2$$\n",
    "\n",
    "Hence our $R$ matrix is:\n",
    "$$\n",
    "R = \n",
    "\\begin{bmatrix}\n",
    "0.7 &0 & 0 \\\\\n",
    "0 & 0.7 & 0 \\\\\n",
    "0 & 0 & 0.0014\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementing the Extended Kalman Filter**\n",
    "\n",
    "Source: https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/\n",
    "\n",
    "Since our model is not linear (the $B$ matrix has a trigonometrical relation with the state varaiable $\\gamma$), we have to implement an Extended Kalman Filter\n",
    "\n",
    "Due to the previous consideration we will have both the $R$ and the $R_{nc}$ to taking in to account the possibility of the Thymio being visible or not. We will use the $R_{nc}$ when the Thymio is not visible and $R$ when it is visible.\n",
    "\n",
    "For the implementation we would refer to the measurements of the position and orientation of the Thymio in the environment obtained using the camera as \n",
    "\n",
    "$$\n",
    "z_{t} =\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Robot control**\n",
    "\n",
    "Initially, a simple P controller was employed for the control law, aiming to minimize the perpendicular distance from the robot to the ideal line. However, this controller led to instability issues. Consequently, we made the decision to switch to a proportional-integral (PI) controller for the angle and a proportional (P) controller for the distance to the optimal path.\n",
    "\n",
    "This controller functions based on the angle error between the desired angle and the current angle of the Thymio robot, along with the perpendicular distance from the Thymio to the line it should be following. Two main modes govern the controller's operation: alignment mode and forward mode.\n",
    "\n",
    "Alignment Mode: In this mode, the controller exclusively addresses the angle error. It is utilized when the robot reaches each point in the path, preparing for the next segment.\n",
    "Forward Mode: In this mode, the controller considers both the angle error and the perpendicular distance to the line. It becomes active when the robot is in motion between two points in the path.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/control.png\" alt=\"Control error diagram\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main**\n",
    "\n",
    "The main function is the core of the project. It is responsible for the initialization of the different modules and the communication between them.\n",
    "\n",
    "The function follows the concept of operations presented in the figure below:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/conOps.png\" alt=\"Cocept of operations\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from src.computerVision import correctPerspectiveStream, findCorners, getPerspectiveMatrix, findThymio, findGlobalObstacles, findGoal\n",
    "from src.kalman import estimatePosition, inverseSpeedConversion, r11, r22, r33, Cl, Cr, R, L\n",
    "from src.pathPlanning import buildGraph\n",
    "from src.robotControl import robotController, checkForObstacles\n",
    "\n",
    "XYMIRROR = False\n",
    "IMAGE_WIDTH = 1920\n",
    "IMAGE_HEIGHT = 1080\n",
    "POSITION_THRESHOLD = 50\n",
    "KIDNAPPING_THRESHOLD = 100\n",
    "KIDNAPPING_TIME = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()\n",
    "\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def motors(l_speed=500, r_speed=500, verbose=False):\n",
    "\n",
    "    global motor_left_target, motor_right_target\n",
    "\n",
    "    l_speed = int(l_speed)\n",
    "    r_speed = int(r_speed)\n",
    "    \n",
    "    motor_left_target = l_speed\n",
    "    motor_right_target = r_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "@tdmclient.notebook.sync_var \n",
    "def horiz_sensor():\n",
    "    global prox_horizontal\n",
    "    return prox_horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "@tdmclient.notebook.sync_var \n",
    "def getVerticalDistance():\n",
    "    global prox_ground_reflected\n",
    "    return prox_ground_reflected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "%%run_python\n",
    "nf_leds_prox_h(0,0,0,0,0,0,0,0) \n",
    "nf_leds_rc(0)\n",
    "nf_leds_temperature(0, 0)\n",
    "nf_leds_bottom_right(0,0,0)\n",
    "nf_leds_bottom_left(0,0,0)\n",
    "nf_leds_top(0,0,0)\n",
    "nf_leds_prox_v(0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: these functions are defined here and not in their respective files because they call the functions that interact with the robot defined above\n",
    "\n",
    "def checkForKidnap():\n",
    "    ver=getVerticalDistance()\n",
    "    if ver[0] < KIDNAPPING_THRESHOLD or ver[1] < KIDNAPPING_THRESHOLD:\n",
    "        print(\"Kidnapped!\")\n",
    "        motors(0,0)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def avoidObstacle(leftOrRight, initTime, currentTime):\n",
    "    motors(0,0)\n",
    "    \n",
    "    tTurn = 1\n",
    "    tStraight = 2\n",
    "    vStraight = 200\n",
    "\n",
    "    if leftOrRight == 'right':\n",
    "        l=-200\n",
    "        r=200\n",
    "    elif leftOrRight == 'left':\n",
    "        l=200\n",
    "        r=-200\n",
    "\n",
    "    motors(l_speed=l, r_speed=r)\n",
    "    if currentTime - initTime < tTurn:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=vStraight, r_speed=vStraight)\n",
    "    if currentTime - initTime < tTurn + tStraight:\n",
    "        return True\n",
    "    \n",
    "\n",
    "    motors(l_speed=-l, r_speed=-r)\n",
    "    if currentTime - initTime < 2*tTurn + tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=vStraight, r_speed=vStraight)\n",
    "    if currentTime - initTime < 2*tTurn + 2.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=-l, r_speed=-r)\n",
    "    if currentTime - initTime < 3*tTurn + 2.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=vStraight, r_speed=vStraight)\n",
    "    if currentTime - initTime < 3*tTurn + 3.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=l, r_speed=r)\n",
    "    if currentTime - initTime < 4*tTurn + 3.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=0, r_speed=0)\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "previousTime = time.time()\n",
    "\n",
    "#state control variables\n",
    "correctedCam = False\n",
    "environmentSetup = False\n",
    "redoPath = False\n",
    "avoidingObstacle = False\n",
    "wasKidnapped = False\n",
    "aligned = False\n",
    "\n",
    "#initializing variables\n",
    "path = []\n",
    "goal = np.array([0,0])\n",
    "lSpeed = 0\n",
    "rSpeed = 0\n",
    "P_k = np.array([[r11,0,0],[0,r22,0],[0,0,r33]])\n",
    "postionHistory = []\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    key = cv2.waitKey(1)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: failed to capture image\")\n",
    "        break\n",
    "\n",
    "    if not correctedCam:\n",
    "        # Map the image to the corner\n",
    "        position, angle, _=findThymio(frame)\n",
    "        estimateState = np.array([position[0],IMAGE_HEIGHT-position[1],angle])\n",
    "        cv2.imshow('Thymio Camera', frame)\n",
    "        if position[0] == -1:\n",
    "            print(\"thymio where are you?\")\n",
    "            continue\n",
    "        try:\n",
    "            centroids = findCorners(frame)\n",
    "            perspectiveMatrix = getPerspectiveMatrix(centroids)\n",
    "            correctedCam = True\n",
    "        except:\n",
    "            print(\"Fail\")\n",
    "            continue\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Correct the perspective of the image\n",
    "    if correctedCam:\n",
    "        frame = correctPerspectiveStream(frame,perspectiveMatrix)\n",
    "        frameToPlot = frame\n",
    "\n",
    "\n",
    "\n",
    "    current_time = time.time()\n",
    "    dt = current_time - previousTime\n",
    "    if dt < 0.15:\n",
    "        time.sleep(0.15-dt)\n",
    "\n",
    "    previousTime = current_time\n",
    "\n",
    "\n",
    "    previousControlVector = inverseSpeedConversion(rSpeed,lSpeed,R,L,Cr,Cl)\n",
    "\n",
    "    position, angle, estimateState, P_k = estimatePosition(frame,previousControlVector,dt,P_k,estimateState)\n",
    "\n",
    "    postionHistory.append(position)\n",
    "\n",
    "\n",
    "\n",
    "    #plot the points in position history in black\n",
    "    for i in range(len(postionHistory)-1):\n",
    "        cv2.circle(frameToPlot, (int(postionHistory[i][0]), int(IMAGE_HEIGHT-postionHistory[i][1])), 5, (0, 0, 0), -1)\n",
    "\n",
    "    #plot the goal\n",
    "    cv2.circle(frameToPlot, (int(goal[0]), int(goal[1])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    #plot the path lines\n",
    "    for i in range(len(path)-1):\n",
    "        cv2.line(frameToPlot,(int(path[i][0]),int(IMAGE_HEIGHT-path[i][1])),(int(path[i+1][0]),int(IMAGE_HEIGHT-path[i+1][1])),(255,255,255),2)\n",
    "        cv2.circle(frameToPlot, (int(path[i][0]), int(IMAGE_HEIGHT-path[i][1])), 5, (255, 255, 255), -1)\n",
    "        \n",
    "    cv2.imshow('Thymio Camera', frameToPlot)\n",
    "    \n",
    "    \n",
    "    if checkForKidnap():\n",
    "        #cambiare le condizioni\n",
    "        print(\"kidnapped\")\n",
    "        lSpeed = 0\n",
    "        rSpeed = 0\n",
    "        wasKidnapped = True\n",
    "        continue\n",
    "\n",
    "\n",
    "    if wasKidnapped:\n",
    "        time.sleep(KIDNAPPING_TIME)\n",
    "        wasKidnapped = False\n",
    "        redoPath = True\n",
    "        continue\n",
    "\n",
    "\n",
    "    if not avoidingObstacle:\n",
    "        leftOrRight = checkForObstacles(horiz_sensor())\n",
    "\n",
    "\n",
    "    if leftOrRight != None or avoidingObstacle:\n",
    "        if not avoidingObstacle:\n",
    "            initTime = time.time()\n",
    "        avoidingObstacle = avoidObstacle(leftOrRight, initTime, time.time())\n",
    "        redoPath = True\n",
    "        continue\n",
    "    \n",
    "\n",
    "    if (key == ord(' ') and environmentSetup == False) or redoPath:\n",
    "        # Capture an image\n",
    "        imagePath = 'capturedImage.jpg'\n",
    "        cv2.imwrite(imagePath, frame)\n",
    "\n",
    "        capturedImage = cv2.imread(imagePath)\n",
    "        position, angle, estimateState, P_k = estimatePosition(capturedImage,previousControlVector,dt,P_k,estimateState)\n",
    "\n",
    "        greenObjects = findGlobalObstacles(capturedImage)\n",
    "\n",
    "        goal = findGoal(capturedImage)\n",
    "        \n",
    "        print(goal)\n",
    "\n",
    "        cv2.imshow('Thymio Camera', capturedImage)\n",
    "        \n",
    "\n",
    "        path=buildGraph(greenObjects,position,goal)\n",
    "        pointCount = 0\n",
    "        print(path)\n",
    "\n",
    "        if redoPath:\n",
    "            redoPath = False\n",
    "            continue\n",
    "        environmentSetup = True\n",
    "        \n",
    "        #cv2.waitKey(0)  # Wait until any key is pressed to close the image window\n",
    "\n",
    "\n",
    "\n",
    "    if environmentSetup:\n",
    "\n",
    "        if np.linalg.norm(position - path[pointCount]) < POSITION_THRESHOLD:\n",
    "            aligned = False\n",
    "            pointCount += 1\n",
    "            motors(0,0)\n",
    "\n",
    "            if pointCount == len(path):\n",
    "                motors(0,0)\n",
    "                print(\"Goal reached\")\n",
    "                break\n",
    "\n",
    "            continue\n",
    "        \n",
    "        if not aligned:\n",
    "            lSpeed,rSpeed,distanceError,angleError = robotController(path[pointCount-1],path[pointCount],position,angle,dt,alignMode=True)\n",
    "            motors(lSpeed,rSpeed)\n",
    "            if np.abs(angleError) < 0.1:\n",
    "                aligned = True\n",
    "                motors(0,0)\n",
    "            continue\n",
    "\n",
    "\n",
    "        lSpeed,rSpeed,distanceError,angleError = robotController(path[pointCount-1],path[pointCount],position,angle,dt,alignMode=False)\n",
    "        motors(lSpeed,rSpeed)\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motors(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sources**\n",
    "\n",
    "\n",
    "Chat-GPT4: https://chat.openai.com/\n",
    "\n",
    "Copilot: https://copilot.github.com/\n",
    "\n",
    "Automatic Addison: https://automaticaddison.com\n",
    "\n",
    "mouhknowsbest (Youtube Channel): https://www.youtube.com/watch?v=aE7RQNhwnPQ\n",
    "\n",
    "Course \"Basics of Mobile Robotics\" by Prof. Francesco Mondada (MICRO-452): https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile_robotics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
