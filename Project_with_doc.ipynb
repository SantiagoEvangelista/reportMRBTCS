{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basics of Mobile Robotics Project** \n",
    "## **EPFL - December 2023**\n",
    "### **Professor: Francesco Mondada**\n",
    "### **Authors:**\n",
    "-Angelo Giovine\n",
    "\n",
    "-Nome2\n",
    "\n",
    "-Nome3\n",
    "\n",
    "-Nome4\n",
    "\n",
    "## **Tables of contents**\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "  - [Modules and libraries required](#Modules-and-libraries-required)\n",
    "  - [Environment](#Environment)\n",
    "- [Vision](#capitolo-2)\n",
    "  - [Sezione 2.1](#sezione-2.1)\n",
    "- [Global Navigation](#capitolo-2)\n",
    "  - [Sezione 2.1](#sezione-2.1)\n",
    "- [Local Navigation](#capitolo-2)\n",
    "  - [Sezione 2.1](#sezione-2.1)\n",
    "- [Filtering](#Filtering:-Extended-Kalman-Filter)\n",
    "  - [Why Kalman?](#Deriving-the-state-space-model-of-the-Thymio-Robot)\n",
    "  - [Deriving the state space model of the Thymio Robot](#Deriving-the-state-space-model-of-the-Thymio-Robot)\n",
    "  - [Dealing with the input](#Dealing-with-the-input)\n",
    "  - [Experimentation to derive $Cl$ and $Cr$](#Experimentation-to-derive-$Cl$-and-$Cr$)\n",
    "  - [Calcultating the covariance matrix of the motion model $Q_t$](#Calcultating-the-covariance-matrix-of-the-motion-model-$Q_t$)\n",
    "  - [Deriving the observation model of the Thymio Robot](#Deriving-the-observation-model-of-the-Thymio-Robot)\n",
    "  - [Calcultating the covariance matrix of the observation model $R_t$ ](#Calcultating-the-covariance-matrix-of-the-observation-model-$R_t$ )\n",
    "  - [testImplementing the Extended Kalman Filter](#Implementing-the-Extended-Kalman-Filter)\n",
    "  - [Sezione 2.1](#sezione-2.1)\n",
    "- [Control law ](#Control-law)\n",
    "- [Main](#Main)\n",
    "- [Conclusions ](#capitolo-2)\n",
    "  - [Sources](#sezione-2.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Demo videos**\n",
    "\n",
    "The optimal path is displayed in white, while the real-time estimated position of the Thymio, obtained through filtering, is indicated by a black dotted line.\n",
    "\n",
    "Full demo + Noraml situation + Kidnapping + Camera obstructed + Local obstacles: https://youtu.be/rz_EOHwGMOw\n",
    "\n",
    "Full demo: https://youtu.be/DTvLO2GhbcM\n",
    "    \n",
    "Normal situation: https://youtu.be/M8M3lAdUMPQ\n",
    "\n",
    "Kidnapping: https://youtu.be/5M4zRgAHsiA\n",
    "\n",
    "Camera obstructed: https://youtu.be/vcZKryRRfx0\n",
    "\n",
    "Local obstacles: https://youtu.be/Q0z2UTtgOtY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "The project is structured around five distinct modules, each tailored to a specific aspect of the Thymio Robot's navigation and operational capabilities:\n",
    "\n",
    "##### Vision Module: \n",
    "This component is responsible for capturing and processing visual data, primarily for map generation and the robot's localization within the map.\n",
    "\n",
    "##### Global Navigation Module: \n",
    "It focuses on plotting a global path for the robot. This is achieved through the implementation of the Dijkstra algorithm on a visibility graph, enabling the robot to plan its route from its current position to the designated goal.\n",
    "\n",
    "##### Local Navigation Module: \n",
    "This module is designed for real-time navigation, particularly in dealing with immediate obstacles. It facilitates local obstacle avoidance and dynamically adjusts the robot's path as new obstacles are detected.\n",
    "\n",
    "##### Filtering Module: \n",
    "The core function of this module is continuous state estimation, primarily achieved through the use of an Extended Kalman Filter. This ensures that the robot has an accurate understanding of its state in relation to the environment, even when the camera is unable to capture visual data.\n",
    "\n",
    "##### Control Law Module: \n",
    "This final module governs the robot's movement and behavior. It includes algorithms for path following and control laws to ensure that the robot adheres to the planned path, and it responds appropriately to changes in the environment, such as being moved unexpectedly (kidnapping detection) or encountering unforeseen obstacles.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modules and libraries required:**\n",
    "\n",
    "-------> INSERIRE TUTTE LE LIBRERE UTILIZZATE E I MODULI, CON SCRIPT PER INSTALLAZIONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Environment**\n",
    "\n",
    "For the chosen environment, we have selected a monochromatic blue background, complemented by 2D green obstacles and a yellow target area. The Thymio robot is identified by a distinctive red \"mask\" applied to it.\n",
    "\n",
    "# INSERIRE QUI FOTO AMBIENTE \n",
    "# INSEIRE QUI FOTO THYMIO\n",
    "\n",
    "\n",
    "In the development of our Mobile Robotics Project, we employed a forward-thinking strategy by incorporating ChatGPT-4's capabilities. Our choice was influenced by ChatGPT-4's sophisticated computational prowess and its adeptness in crafting intricate geometric figures. We primarily utilized the model to construct a variety of polygons, which functioned as fixed obstacles on our map. We observed that while the model could produce a broad spectrum of shapes, from simple triangles to elaborate polygons, it encountered difficulties in creating a PDF with non-overlapping shapes. Additionally, it was challenged in randomly generating polygons, often resulting in blank outputs or error messages. Consequently, we intervened by specifying the dimensions, forms, and hues of the polygons. As a result, our map was not entirely AI-generated but rather a collaborative effort, blending our specific polygon choices with GPT-4's assembly skills. This experience once again underscored the limitations of a Large Language Model (LLM) in tasks demanding significant creativity.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/GPT_error.png\" alt=\"GPT error message\" width=\"450\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/GPT_env.png\" alt=\"polygons generated by GPT4 \" width=\"550\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sezione 2.1\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/iPhone.jpg\" alt=\"very bad image of the concept\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Global Navigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sezione 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Local Navigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sezione 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Filtering: Extended Kalman Filter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Kalman?**\n",
    "\n",
    "During the course we have encountered different Bayes filter to estimate the state of a robot in a given environment. For example, we could have used a particle filter, but it is more computationally expensive and it is not necessary for this application. For our task we have choosen the Kalman filter, a recursive filter that estimates the state of a system from a series of noisy measurements. It is a very efficient algorithm that can be used to solve the localization problem of a robot, but has a crucial assumption about the distribution of the noise. In fact, it assumes that the noise is Gaussian distributed, which is not always the case in real life. However, in differential robots, like the Thymio, odometry is primarily based on the velocity sensors of the motors to calculate the robot's position and orientation ($x, y, \\gamma$). The errors in odometry can arise from various sources:\n",
    "- Sensor Measurement Errors: The sensors might not be perfectly accurate or might exhibit some drift over time.\n",
    "- Mechanical Misalignments: Slight imperfections in the robot's construction or wear in components can cause discrepancies.\n",
    "- Differential Motion Transmission Variances: Factors like friction, tire pressure, or differences in the movement surface can affect how motion is transmitted from the motors to the wheels.\n",
    "- Slippage Error: The wheels may slip or lose traction, leading to inaccurate distance measurements.\n",
    "\n",
    "Since each of these error sources contributes small, independent increments to the overall odometry error. According to the Central Limit Theorem, when a large number of small, independent effects are summed, their overall distribution tends to approach a Gaussian distribution. This implies that even if each individual source of error might not be Gaussian, their cumulative effect tends towards a normal distribution. \n",
    "\n",
    "Since the odometry noise can be assumed to be Gaussian distributed, the Kalman filter, that's easy to implement, is a good choice for our application.\n",
    "\n",
    "Once again we have decied to collaborate with LLM. This time we wanted to generate an image suitable for this paragraph, but even if we were precise about the contest ChatGPT-4 was able to produce an image not appropriated for this paragraph, the image is not scientific or accurated but we decided to use it anyway, to highlight another limit of LLM.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/GPT_Kalman.png\" alt=\"very bad image of the concept\" width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deriving the state space model of the Thymio Robot**\n",
    "\n",
    "Source: https://automaticaddison.com/how-to-derive-the-state-space-model-for-a-mobile-robot/ \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/state_space_.png\" alt=\"2D image of a differential drive robot\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "First of all, we need to define the state and input variables. We will use the following state variables: $x$ and $y$ for the position, and $\\gamma$ for the orientation, since we can control our Thyimio in speed, we can define our input variables $v$ forward velocity, and $\\omega$ angular velocity . After some simple trigonometric analysis is easy to define the state vector $s$ is defined as follows: \n",
    "$$\n",
    "s_{t} =\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{t-1} + v_{t-1}\\cos\\gamma_{t-1} \\cdot \\Delta t \\\\\n",
    "y_{t-1} + v_{t-1}\\sin\\gamma_{t-1} \\cdot \\Delta t \\\\\n",
    "\\gamma_{t-1} + \\omega_{t-1} \\cdot \\Delta t\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "f_1 \\\\\n",
    "f_2 \\\\\n",
    "f_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence our input vector $u$ is defined as follows:\n",
    "\n",
    "$$\n",
    "u_{t} = \n",
    "\\begin{bmatrix}\n",
    "    v_{t} \\\\ \n",
    "     \\omega_{t}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We would now define our state space model as our system is linaer so in this form:  $s_{t}= A_{t-1}s_{t-1} + B_{t-1}u_{t-1}$, where $A_{t-1}$ is the state matrix and $B_{t-1}$ is the input matrix. But our original system is not linear. However, we can approximate our system by using the Jacobian matrix.\n",
    "Our Thymio moves only when it receives instructions to rotate its wheels. As a result, in this scenario, the $A_{t-1}$ matrix is an identity matrix (linear relation). For the $B_{t-1}$ matrix we can use the Jacobian matrix, which is defined as follows:\n",
    "\n",
    "$$\n",
    "B_{t-1} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial v_{t-1}} & \\frac{\\partial f_1}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_2}{\\partial v_{t-1}} & \\frac{\\partial f_2}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_3}{\\partial v_{t-1}} & \\frac{\\partial f_3}{\\partial \\omega_{t-1}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "\\sin(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "0 & \\Delta t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "If we consider also the process noise $ w_{t} = [ w^1_{t}, w^2_{t}, w^3_{t} ] $\n",
    " , we can define our state space model as follows:\n",
    "$$s_{t}= A_{t-1}s_{t-1} + B_{t-1}u_{t-1} +w_{t-1}$$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{t-1} \\\\\n",
    "y_{t-1} \\\\\n",
    "\\gamma_{t-1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "\\sin(\\gamma_{t-1} \\cdot \\Delta t) & 0 \\\\\n",
    "0 & \\Delta t\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{t-1} \\\\\n",
    "\\omega_{t-1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w^1_{t-1} \\\\\n",
    "w^2_{t-1} \\\\\n",
    "w^3_{t-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that we are assuming that the process noise is zero-mean Gaussian noise with covariance matrix $Q_k$, which is computed after some experimentation explained in the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dealing with the inputs** \n",
    "\n",
    "Source: https://www.youtube.com/watch?v=aE7RQNhwnPQ\n",
    "\n",
    "If we notice, our state space model is controlled in $v$ forward velocity, and $\\omega$ angular velocity but in reality we can only control the speed of the two motors $v_{right}$, and $v_{left}$ of our Thymio. So we need to find a way to convert the speed of the robot $v$ and $\\omega$ into $v_{right}$, and $v_{left}$. We can do this by using the following equations:\n",
    "\n",
    "$v_{right} = \\frac{2v+\\omega L}{2R}$ , $v_{left} = \\frac{2v-\\omega L}{2R}$\n",
    "\n",
    "Where $L$ is the distance between the two wheels and $R$ is the radius of the wheels.\n",
    "\n",
    "In order to control the Thymio we must convert the speed of the two motors from the Thymio unit system to $v_{right}$, and $v_{left}$  we can do this buy using the convertion factor $Cr$ and $Cl$ calculated through experimentation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_converison(v,omega,R,L,Cr,Cl):\n",
    "    # v: [mm/s] input variable 1\n",
    "    # omega: [rad/s] input variable 2\n",
    "    # R: [mm] wheel radius of the Thymio\n",
    "    # L: [mm] distance between the wheels of the Thymio\n",
    "    # Cr: [(s/rad)*Thymio units] convertion factor from rad/s to Thymio speed unit\n",
    "    # Cl: [(s/rad)*Thymio units] convertion factor from rad/s to Thymio speed unit\n",
    "    # return: [Thymio unit, Thymio unit]\n",
    "    v_r = ((2*v + omega*L)/(2*R))*Cr\n",
    "    v_l = ((2*v - omega*L)/(2*R))*Cl\n",
    "    return v_r, v_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_speed_conversion(v_r, v_l, R, L, Cr, Cl):\n",
    "    # v_r: [Thymio unit] input variable 1\n",
    "    # v_l: [Thymio unit] input variable 2\n",
    "    # R: [mm] wheel radius of the Thymio\n",
    "    # L: [mm] distance between the wheels of the Thymio\n",
    "    # Cr: [(s/rad)*Thymio units] convertion factor from rad/s to Thymio speed unit\n",
    "    # Cl: [(s/rad)*Thymio units] convertion factor from rad/s to Thymio speed unit\n",
    "    v = (R/2)*(v_r/Cl + v_l/Cr)\n",
    "    omega = (R/L)*(v_r/Cl - v_l/Cr)\n",
    "    return v, omega\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experimentation to derive $Cl$ and $Cr$**\n",
    "\n",
    "In order to use the $Cl$ and $Cr$ values in the program, we need to derive them from the experimental data. This section contains the collected data and the results. First we have marked each wheel of the Thymio robot then we have run the robot at a fixed speed $spd = 200$  $Thymio units$ and measured the number of revolution of each wheel in a fixed time interval $t$. We have repeated this experiment 10 times and calculated our average values for $Cl$ and $Cr$.\n",
    "Since we have noticed that $Cl$ and $Cr$ are not proportional with the Thymio motors speed, we have decided to use the speed used to derive this coefficent as the constant speed input in our controller.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Thymio_wheel.jpeg\" alt=\"Description of Thymio wheel\" width=\"650\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/Cl_Cr.png\" alt=\"Description of second image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "$$Cl = 69.33821285 \\frac{s}{rad}Thymio units,Cr = 70.74580573 \\frac{s}{rad}Thymio units$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calcultating the covariance matrix of the motion model $Q_t$**\n",
    "\n",
    "\n",
    "$$\n",
    "Q_t = \n",
    "\\begin{bmatrix}\n",
    "Var(x_t) & Cov(x_t, y_t) & Cov(x_t, \\gamma_t) \\\\\n",
    "Cov(y_t, x_t) & Var(y_t) & Cov(y_t, \\gamma_t) \\\\\n",
    "Cov(\\gamma_t, x_t) & Cov(\\gamma_t, y_t) & Var(\\gamma_t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To implement a Kalman filter, we first need to know the variance of the motion model. \n",
    "While my odometry system provides only the velocity of the two motors, we can use the geometry of the Thymio to calculate the position and orientation of the robot, hence: $$x=f_1(v_r,v_l) = \\frac{R}{2} (v_r + v_l) \\cos(\\gamma) \\Delta t$$ $$y=f_2(v_r,v_l) = \\frac{R}{2} (v_r + v_l) \\sin(\\gamma) \\Delta t$$ $$\\gamma=f_3(v_r,v_l) =\\frac{R}{L} (v_r-v_l) \\Delta t$$ where $v_r$ and $v_l$ are the velocity of the two motors, $R$ is the radius of the wheel and $L$ is the distance between the two wheels . Now we have to take in account the error propagation, so we can calculate $$Var(x) = (\\frac{\\partial f_1}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l})^2 Var(v_l) +2(\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) $$ $$Var(y) = (\\frac{\\partial f_2}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_2}{\\partial v_l})^2 Var(v_l) +2(\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) $$ $$Var(\\gamma) = (\\frac{\\partial f_3}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_3}{\\partial v_l})^2 Var(v_l) +2 (\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l)  $$ \n",
    "\n",
    "We can consider $v_r$ and $v_l$ uncorrelated so after deriving the partial derivatives we can calculate the diagonal terms of $Q_t$ as follows:\n",
    "\n",
    "$$Var(x) =  (\\frac{R}{2} \\cos(\\gamma) \\Delta t)^2 (Var(v_r)+Var(v_l))$$ \n",
    "$$Var(y) = (\\frac{R}{2} \\sin(\\gamma) \\Delta t)^2 (Var(v_r)+Var(v_l))$$ \n",
    "$$Var(\\gamma) =  (\\frac{R}{L}\\Delta t)^2 (Var(v_r)+Var(v_l))$$\n",
    "\n",
    "Next we need to calculate the off-diagonal terms of $Q_t$, we can do it again by taking in account the error propagation:\n",
    "$$ Cov(x,y) = Cov(y,x)  = (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_2}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_2}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_2}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_2}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "$$ Cov(x,\\gamma) = Cov(\\gamma,x) = (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "$$ Cov(y,\\gamma) = Cov(\\gamma,y) = (\\frac{\\partial f_2}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_2}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_2}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_2}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "\n",
    "After deriving the partial derivatives we obatain:\n",
    "\n",
    "$$ Cov(x,y) = \\frac{(R  \\Delta t)}{4}^2(\\cos(\\gamma)\\sin(\\gamma))(Var(v_r)+Var(v_l)) $$\n",
    "$$ Cov(x,\\gamma) = \\frac{(R \\Delta t)^2}{2L} \\cos(\\gamma) (Var(v_r)-Var(v_l) )$$\n",
    "$$ Cov(y,\\gamma) = \\frac{(R \\Delta t)^2}{2L} \\sin(\\gamma) (Var(v_r)-Var(v_l)) $$\n",
    "\n",
    "Now that we have our covariance matrix $Q_k$ formulation, we can calulate the parameters $Var(v_r)$ and $Var(v_l)$ by experimentation. By simply collecting data from our Thymio proceding at a constant speed of $200$ $Thymiounits$, we have obtained the variance of the two motors, in $(rad/s)^2$ \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/motor_speed.png\" alt=\"Description of the image\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "$$Var(v_r) = 0.013997244458988999 (rad/s)^2$$\n",
    "$$Var(v_l) = 0.007137898281767495 (rad/s)^2$$\n",
    "\n",
    "After some simulation for different values of $\\gamma$ from $0°$ to $180°$ we have found that the matrix $Q_t$ is diagonal.\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Q.png\" alt=\"Description of the image\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "To ensure that the Kalman filter minimizes the variance of the error between the actual state and the predicted state, it is necessary to have a $Q$ matrix that is not time-varying ($Q_t = Q$). To achieve this, it is essential to find a $Q$ matrix that takes scalar inputs. Through simulation, we have selected a $Q$ matrix with values that maximize the variance values, thereby obtaining a conservative estimate of the matrix.\n",
    "\n",
    "$$\n",
    "Q =\n",
    "\\begin{bmatrix}\n",
    "0.20000 & 0.00000 & 0.00000 \\\\\n",
    "0.00000 & 0.20000 & 0.00000 \\\\\n",
    "0.00000 & 0.00000 & 0.00070\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "Another crucial aspect to consider is that in our model the input contribution ($ B_{t-1}u_{t-1}$) represents a deterministic quantity. As a result, our current $Q$ matrix accounts solely for the state error, excluding any input error. A common practical approach in control theory is to incorporate the input error into an experimental variable, $\\alpha$, which requires experimental tuning. Consequently, our actual $Q$ matrix becomes $Q$ =$\\alpha  Q$, effectively encompassing the error associated with the input as well. After some tuning we have found that a good matrix $Q$ for our application is:\n",
    "\n",
    "$$\n",
    "Q = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0. \\\\\n",
    "0& 2 & 0 \\\\\n",
    "0 & 0 & 0.07\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deriving the observation model of the Thymio Robot**\n",
    "\n",
    "Source: https://automaticaddison.com/how-to-derive-the-observation-model-for-a-mobile-robot/\n",
    "\n",
    "Now that we have a linear state space model of our Thyimio that includes also the process noise, we need an observation model in order to be able to implement the Kalman filter. \n",
    "\n",
    "An observation model describes how expected sensor outputs $y_t$ are related to the state vector $s_t$, considering also a vector of observation noise $ \\nu_{t} = [ \\nu^1_{t}, \\nu^2_{t}, \\nu^3_{t} ] ^T$\n",
    "with zero mean and covariance matrix $R$ (which is computed after some experimentation explained in the next section). \n",
    "The observation model is defined as follows: \n",
    "$$y_t = H_t s_t + \\nu_t$$ \n",
    "Since we are able to detect the position and the orientation of the Thyimio in the environment using camera, our matrix $H$ is the identity, hence we can define our observation model as follows:\n",
    "\n",
    "$$\n",
    "y_{t} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\nu^1_{t}\\\\\n",
    "\\nu^2_{t}\\\\\n",
    "\\nu^3_{t}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calcultating the covariance matrix of the observation model $R_t$**\n",
    "\n",
    "The matrix $R_t$ is obtain by experimentation, and contain the variance of the observation noise, in our case the variance of the position and orientation of the Thymio in the environment, we can denote the observation vector as follows:\n",
    "\n",
    "$$\n",
    "z_{t} =\n",
    "\\begin{bmatrix}\n",
    "x^o_{t} \\\\\n",
    "y^o_{t} \\\\\n",
    "\\gamma^o_{t}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "We can calculate the variance of the observation and it will be:\n",
    "\n",
    "$$\n",
    "R_t = \n",
    "\\begin{bmatrix}\n",
    "Var(x^o) & Cov(x^o, y^o) & Cov(x^o, \\gamma^o) \\\\\n",
    "Cov(y^o, x^o) & Var(y^o) & Cov(y^o, \\gamma^o) \\\\\n",
    "Cov(\\gamma^o, x^o) & Cov(\\gamma^o, y^o) & Var(\\gamma^o)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that those term are different from the ones of the motion model, because the observation noise is releted with our camera and our image vision approach, not with the Thymio itself. This matrix changes in relation with the visibility of our Thymio, for simplicity we can assume two different scenarios, when the Thymio is visible and when it is not, so we can define just 2 matrix $R$ and $R_{nc}$.If we can not detect it the incertence grows to infinity, so we can define a matrix $R_{nc}$, in this case we set the variance of the observation noise to infinity:\n",
    "$$\n",
    "R_{nc} = \n",
    "\\begin{bmatrix}\n",
    "\\infty &0 & 0 \\\\\n",
    "0 & \\infty & 0 \\\\\n",
    "0 & 0 & \\infty \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now I can compute $R_t = R$ through experimentation, first we need to create an A4 paper as a calibration grid, ensuring the most accurate positioning of the Thymio robot before detecting its location and orientation. The A4 paper was easly generated using ChatGPT-4. The LLM after getting as input the shapes and the dimension needed for the grid was able to generate the following file:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/square_design.png\" alt=\"Description of the image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Then we have computed the required variance mesuring the delta between the measure of the Thymio provided by the camera and the real position of the Thymio. We have repeated this 10 times and calculated our average values for $Var(x^o)$, $Var(y^o)$ and $Var(\\gamma^o)$. To be consistent with the units of the motion model, we have converted the camera measure from pixels to millimiters.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/R_exp.jpg\" alt=\"Description of the image\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$Var(x^o) = 0.7 (mm)^2$$\n",
    "$$Var(y^o) = 0.7 (mm)^2$$\n",
    "$$Var(\\gamma^o) = 0.0014 (rad)^2$$\n",
    "\n",
    "Hence our $R$ matrix is:\n",
    "$$\n",
    "R = \n",
    "\\begin{bmatrix}\n",
    "0.7 &0 & 0 \\\\\n",
    "0 & 0.7 & 0 \\\\\n",
    "0 & 0 & 0.0014\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementing the Extended Kalman Filter**\n",
    "\n",
    "Source: https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/\n",
    "\n",
    "Since our model is not linear (the $B$ matrix has a trigonometrical relation with the state varaiable $\\gamma$), we have to implement an Extended Kalman Filter\n",
    "\n",
    "Due to the previous consideration we will have both the $R$ and the $R_{nc}$ to taking in to account the possibility of the Thymio being visible or not. We will use the $R_{nc}$ when the Thymio is not visible and $R$ when it is visible.\n",
    "\n",
    "For the implementation we would refer to the measurements of the position and orientation of the Thymio in the environment obtained using the camera as \n",
    "\n",
    "$$\n",
    "z_{t} =\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t}\n",
    "\\end{bmatrix} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "## Code insipiration from Addison Sears-Collins\n",
    "# https://automaticaddison.com\n",
    "##############################################################################################################\n",
    "import numpy as np\n",
    "\n",
    "R = 20 #mm\n",
    "L = 105 #mm\n",
    "var_vr = 0.1 #da cambiare\n",
    "var_vl = 0.1 #da cambiare\n",
    " \n",
    "# State matrix A\n",
    "\n",
    "A_k_minus_1 = np.array([[1.0,0,0],[0,1.0,0],[0,0,1.0]])\n",
    " \n",
    "# Input matrix B\n",
    "def getB(yaw, deltak):\n",
    "    B = np.array([  [np.cos(yaw)*deltak, 0],[np.sin(yaw)*deltak, 0],[0, deltak]])\n",
    "    return B\n",
    "\n",
    "# Process noise w\n",
    "process_noise_w_k_minus_1 = np.array([0,0,0])\n",
    "     \n",
    "# State model noise covariance matrix Q_k\n",
    "q11 = 2 \n",
    "q22 = 2\n",
    "q33 = 0.07\n",
    "Q_k = np.array([[q11, 0, 0],[0, q22, 0],[0, 0, q33]])                 \n",
    "# Measurement matrix H_k\n",
    "H_k = np.array([[1.0,0,0],[0,1.0,0],[0,0,1.0]])\n",
    "                         \n",
    "# Sensor measurement noise covariance matrix R_k\n",
    "r11 = 0.7 #da cambiare\n",
    "r22 = 0.7 #da cambiare\n",
    "r33 = 0.0014 #da cambiare\n",
    "R_k = np.array([[r11,0,0],[0,r22,0],[0,0,r33]])\n",
    "\n",
    "# Sensor measurement noise covariance matrix R_k_nc\n",
    "R_k_nc = np.array([[np.inf,np.inf,np.inf],[np.inf,np.inf,np.inf],[np.inf,np.inf,np.inf]])\n",
    "                 \n",
    "# Sensor noise v\n",
    "sensor_noise_v_k = np.array([0,0,0])\n",
    "\n",
    "# Initial state covariance matrix\n",
    "# choose how to initialize it\n",
    "P_k_minus_1 = np.array([[r11,0,0],[0,r22,0],[0,0,r33]]) \n",
    "                                               \n",
    "                                               \n",
    "\n",
    "#camera_vision = True means that the camera is used to estimate the position\n",
    "#camera_vision = False means that the camera is covered so it is not used to estimate the position\n",
    " \n",
    "def ekf(z_k_observation_vector, state_estimate_k_minus_1,control_vector_k_minus_1, P_k_minus_1, dk,camera_vision):\n",
    "\n",
    "    # Prediction step\n",
    "\n",
    "    #Prediction of the state estimate\n",
    "    state_estimate_k = A_k_minus_1 @ (state_estimate_k_minus_1) + (getB(state_estimate_k_minus_1[2],dk)) @ (control_vector_k_minus_1) + (process_noise_w_k_minus_1)\n",
    "    print(state_estimate_k)             \n",
    "    # Predicton the state covariance estimate \n",
    "    yaw = state_estimate_k[2]\n",
    "    P_k = A_k_minus_1 @ P_k_minus_1 @ A_k_minus_1.T + Q_k\n",
    "         \n",
    "    # Update step\n",
    "\n",
    "    measurement_residual_y_k = z_k_observation_vector - ((H_k @ state_estimate_k) + (sensor_noise_v_k))\n",
    "             \n",
    "    # Calculate the measurement residual covariance\n",
    "    if camera_vision == True:\n",
    "        S_k = H_k @ P_k @ H_k.T + R_k\n",
    "    else:\n",
    "        S_k = H_k @ P_k @ H_k.T + R_k_nc\n",
    "         \n",
    "    # Calculate the near-optimal Kalman gain\n",
    "\n",
    "    K_k = P_k @ H_k.T @ np.linalg.pinv(S_k)\n",
    "         \n",
    "    # Calculate an updated state estimate for time k\n",
    "    state_estimate_k = state_estimate_k + (K_k @ measurement_residual_y_k)\n",
    "    print(state_estimate_k) \n",
    "    # Update the state covariance estimate for time k\n",
    "    P_k = P_k - (K_k @ H_k @ P_k)\n",
    "     \n",
    "    # Return the updated state and covariance estimates\n",
    "    return state_estimate_k, P_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Control law RICONTROLLARE FORSE INUTILE SPIEGARE IL CODICE COSI' IN DETTAGLIO**\n",
    "\n",
    "For the control law we have choose to use a simple PI controller, that works on the angle error between the desired angle and the current angle of the Thymio. The idea is the following: first it calculates the angle of the line between pointInitial and pointFinal, then determines the error between the desired angle of the line and the current angle of the robot. It uses proportional (P) and integral (I) control to calculate the differential speed needed to correct the angular error. Then it computes the perpendicular distance from the current position to the line between pointInitial and pointFinal and\n",
    "implements proportional (P), control for correcting the position relative to the desired line. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIDcontrol(pointInitial,pointFinal,position,angle,dt,alignMode):\n",
    "\n",
    "    #make controller for the angle\n",
    "    angleOfLine = np.arctan2(pointFinal[1]-pointInitial[1],pointFinal[0]-pointInitial[0])# - np.pi\n",
    "\n",
    "\n",
    "    if angleOfLine < 0:\n",
    "        angleOfLine = angleOfLine + 2*np.pi\n",
    "    #print(angleOfLine)\n",
    "\n",
    "    angleError = angleOfLine-angle\n",
    "\n",
    "\n",
    "\n",
    "    if angleError > np.pi:\n",
    "        angleError -= 2*np.pi\n",
    "    elif angleError < -np.pi:\n",
    "        angleError += 2*np.pi\n",
    "\n",
    "\n",
    "    Kangle = 50\n",
    "    KIangle = 5\n",
    "\n",
    "    differentialSpeed = Kangle*angleError + KIangle*angleError*dt\n",
    "\n",
    "    distanceToLine = getDistance(position,pointInitial,pointFinal)\n",
    "\n",
    "    #print(\"distance error:\" + str(distanceToLine) + \" angle error:\" + str(angleError))\n",
    "\n",
    "    Kdistance = 0.3\n",
    "    KIdistance = 0.1\n",
    "    KDdistance = 0.1\n",
    "    \n",
    "\n",
    "    posCorrection = Kdistance*distanceToLine\n",
    "\n",
    "    constantSpeed = 200\n",
    "\n",
    "\n",
    "    if alignMode == True:\n",
    "        constantSpeed = 0\n",
    "        posCorrection = 0\n",
    "        posCorrection = 0\n",
    "\n",
    "    l_speed = -differentialSpeed - posCorrection + constantSpeed\n",
    "    r_speed = differentialSpeed + posCorrection + constantSpeed\n",
    "\n",
    "\n",
    "    return l_speed, r_speed, distanceToLine, angleError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "\n",
    "Chat-GPT4: https://chat.openai.com/\n",
    "\n",
    "Copilot: https://copilot.github.com/\n",
    "\n",
    "Automatic Addison: https://automaticaddison.com\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile_robotics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
